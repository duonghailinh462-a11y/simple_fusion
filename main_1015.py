#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SDKç‰ˆå¤šæ‘„åƒå¤´èåˆç³»ç»Ÿ - é‡æ„ç‰ˆ
èŒè´£åˆ†ç¦»: 
1. å­è¿›ç¨‹ (yolov5_SDK): ä»…è´Ÿè´£è§†é¢‘è¯»å–ã€SDKæ¨ç†ã€ç»“æœå…¥é˜Ÿåˆ— 
2. ä¸»è¿›ç¨‹ (main): è´Ÿè´£è·Ÿè¸ª(BYTETracker)ã€åŒºåŸŸè¿‡æ»¤ã€è·¨æ‘„åƒå¤´èåˆã€å¸§åŒæ­¥ 
"""
import threading
from queue import Queue
import os
import sys
import time
import signal
import multiprocessing
import copy
import json
import re
from collections import defaultdict, deque
from statistics import mean, median
sys.path.append('/usr/local/lynxi/sdk/sdk-samples/python')

import numpy as np
import cv2
from ctypes import *
import argparse
import struct
import ctypes
from dataclasses import dataclass
from typing import List, Tuple, Optional, Set, Dict

# å¯¼å…¥SDKç›¸å…³æ¨¡å—
import pycommon.common as common
import pylynchipsdk as sdk
from pycommon.infer_process import *
from pycommon.callback_data_struct import *
from pycommon.dump_json import *
from ByteTrack.optimized_byte_tracker import OptimizedBYTETracker as BYTETracker

# å¯¼å…¥RTSPå’ŒMQTTç›¸å…³æ¨¡å— (æ–°å¢)
try:
    from rtsp_reader import RTSPStreamReader
    from mqtt_publisher import MqttPublisher  
    from config_reader import ConfigReader
    RTSP_MQTT_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  æ— æ³•å¯¼å…¥RTSP/MQTTæ¨¡å—: {e}, å°†ä½¿ç”¨åŸæœ‰çš„æœ¬åœ°è§†é¢‘æ¨¡å¼")
    RTSP_MQTT_AVAILABLE = False
# åˆ›å»ºå…±äº«å¸ƒå°”å€¼ç”¨äºåœæ­¢è¿è¡Œçº¿ç¨‹
cancel_flag = multiprocessing.Value('b', False)
# --- å¼‚æ­¥JSONä¿å­˜å™¨ç±» (åœ¨ CrossCameraFusion ç±»ä¹‹å‰æ·»åŠ ) ---
class AsyncJsonSaver:
    """å¼‚æ­¥JSONä¿å­˜å™¨ï¼Œä½¿ç”¨åå°çº¿ç¨‹é¿å…é˜»å¡ä¸»çº¿ç¨‹"""
    
    def __init__(self, num_workers=1):
        self.save_queue = Queue(maxsize=10)  # é™åˆ¶é˜Ÿåˆ—å¤§å°é˜²æ­¢å†…å­˜æº¢å‡º
        self.workers = []
        self.stop_event = threading.Event()
        
        # å¯åŠ¨åå°å·¥ä½œçº¿ç¨‹
        for i in range(num_workers):
            worker = threading.Thread(target=self._worker_loop, daemon=True, name=f"JsonSaver-{i}")
            worker.start()
            self.workers.append(worker)
    
    def _worker_loop(self):
        """åå°å·¥ä½œçº¿ç¨‹å¾ªç¯"""
        while not self.stop_event.is_set():
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼Œè¶…æ—¶é˜²æ­¢æ­»é”
                task = self.save_queue.get(timeout=0.1)
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                
                output_file, data = task
                self._write_json_file(output_file, data)
                
            except:
                # é˜Ÿåˆ—è¶…æ—¶æˆ–å…¶ä»–é”™è¯¯ï¼Œç»§ç»­ç­‰å¾…
                continue
    
    def _write_json_file(self, output_file: str, data: dict):
        """å®é™…å†™å…¥JSONæ–‡ä»¶çš„æ“ä½œ"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ å¼‚æ­¥JSONä¿å­˜å¤±è´¥ ({output_file}): {e}")
    
    def save_async(self, output_file: str, data: dict):
        """éé˜»å¡åœ°æäº¤ä¿å­˜ä»»åŠ¡"""
        try:
            # ä¸é˜»å¡ï¼Œå¦‚æœé˜Ÿåˆ—æ»¡åˆ™ä¸¢å¼ƒæœ€æ–°çš„ä¿å­˜ä»»åŠ¡
            self.save_queue.put_nowait((output_file, data))
        except:
            # é˜Ÿåˆ—æ»¡æ—¶é™é»˜å¤„ç†
            pass
    
    def shutdown(self):
        """å…³é—­å¼‚æ­¥ä¿å­˜å™¨å¹¶ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ"""
        # ç­‰å¾…é˜Ÿåˆ—æ¸…ç©º
        while not self.save_queue.empty():
            time.sleep(0.01)
        
        # å‘é€åœæ­¢ä¿¡å·
        self.stop_event.set()
        
        # ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹ç»“æŸ
        for worker in self.workers:
            worker.join(timeout=2.0)

# --- é…ç½®ç±»  ---
@dataclass
class Config:
    IMAGE_WIDTH: int = 1920
    IMAGE_HEIGHT: int = 1080
    FPS: int = 25
    TRACK_THRESH: float = 0.3
    MATCH_THRESH: float = 0.6
    MIN_FRAMES_THRESHOLD: int = 10
    TIME_WINDOW: int = 80
    TEMPORAL_WINDOW_MAX: int = 225
    BASE_SPATIAL_THRESHOLD: float = 400.0
    IOU_THRESHOLD: float = 0.5
    TOLERANCE_FRAMES: int = 60
    # æ–°å¢ï¼šåƒç´ Yå€¼é˜ˆå€¼ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦åœ¨åº•éƒ¨åŒºåŸŸï¼‰
    PIXEL_BOTTOM_THRESHOLD: int = 700  # Yå€¼ä¸‹é™
    PIXEL_TOP_THRESHOLD: int = 1080    # Yå€¼ä¸Šé™
    # æ–°å¢ï¼šèåˆæ—¶é—´çª—å£
    FUSION_TIME_WINDOW: int = 60
    VEHICLE_CLASSES = ['mini_truck','truck','bus','van','car','person','bike','electric_vehicle',
    'tricycle','engineer','ambulance','fireEngine','schoolBus','tanker','muckTruck',
    'concreteTruck','policeCar']
    EXCLUDE_CLASSES = ["person", "electric_vehicle", "bike", "tricycle"]
    SIMILAR_CLASSES = {
        'mini_truck': ['truck', 'van', 'car'],
        'truck': ['mini_truck', 'van', 'car'],
        'van': ['mini_truck', 'truck', 'car'],
        'car': ['van', 'mini_truck', 'truck'],
        'bus': ['truck', 'van', 'car'],
    }

# çŸ©é˜µå’ŒåŒºåŸŸé…ç½®
CAMERA_MATRICES = {
    1: np.array([[-10.5149, 222.4408462, -9790.39534446],
                 [-9.19971325, 69.92947807, 19308.61520761],
                 [-0.00395588, 0.18180162, 1.0]], dtype=np.float64),
    2: np.array([[-1.27499084, -13.21167486, -554.98403431],
                 [-1.56665505, -18.83507418, 3226.90502457],
                 [-0.00079458, -0.019352, 1.0]], dtype=np.float64),
    3: np.array([[-0.00256905, -7.80437003, -464.08522659],
                 [-0.64679995, -4.01640045, -320.34097331],
                 [-0.00060155, -0.01122723, 1.0]], dtype=np.float64)
}

BEV_TO_GEO_MATRIX = np.array([
  [-0.025828551721,  0.026850072654, 113.582941425078],
  [-0.005351012238,  0.005561209192,  23.531419339129],
  [-0.000227406715,  0.000236388495,   1.0]
])
PUBLIC_AREA_BEV = np.array([[1075, 569], [1106, 604], [850, 761], [825, 727]], dtype=np.int32)

# YOLOv5 ç±»åˆ«åç§°
NAMES = [
    'mini_truck','truck','bus','van','car','person','bike','electric_vehicle',
    'tricycle','engineer','ambulance','fireEngine','schoolBus','tanker','muckTruck',
    'concreteTruck','policeCar'
]

# --- æ–°å¢ï¼šæ•°æ®ç»“æ„ (ä»0915èåˆé€»è¾‘ç§»æ¤) ---
@dataclass
class GlobalTarget:
    """å…¨å±€ç›®æ ‡æ•°æ®ç±»"""
    global_id: int
    camera_id: int
    local_id: int
    class_name: str
    bev_trajectory: List[Tuple[float, float]]
    pixel_trajectory: List[Tuple[int, int]]
    last_seen_frame: int
    is_active: bool
    fusion_alpha: float = 0.2
    is_in_fusion_zone: bool = False
    confidence_history: List[float] = None
    fusion_entry_frame: int = -1
    
    def __post_init__(self):
        if self.confidence_history is None:
            self.confidence_history = []

@dataclass
class LocalTarget:
    """æœ¬åœ°ç›®æ ‡æ•°æ®ç±»"""
    local_id: int
    camera_id: int
    class_name: str
    current_bev_pos: Tuple[float, float]
    current_pixel_pos: Tuple[int, int]
    confidence: float
    is_in_fusion_area: bool
    matched_global_id: Optional[int] = None
    detection_box: List[int] = None
    fusion_entry_frame: int = -1
    
    def __post_init__(self):
        if self.detection_box is None:
            self.detection_box = []

class LocalTrackBuffer:
    """æœ¬åœ°è½¨è¿¹ç¼“å†²åŒº - ç»´æŠ¤æ¯ä¸ªæ‘„åƒå¤´çš„trackè½¨è¿¹å†å²"""
    def __init__(self, max_history: int = 30):
        self.max_history = max_history
        self.tracks: Dict[int, Dict[int, List[Tuple[float, float]]]] = defaultdict(lambda: defaultdict(list))
        self.pixel_tracks: Dict[int, Dict[int, List[Tuple[int, int]]]] = defaultdict(lambda: defaultdict(list))
        self.assigned_global_ids: Dict[int, Dict[int, int]] = defaultdict(dict)
        self.track_classes: Dict[int, Dict[int, str]] = defaultdict(dict)
    
    def __len__(self):
        """è¿”å›trackæ€»æ•°"""
        total = 0
        for camera_id in self.tracks:
            total += len(self.tracks[camera_id])
        return total
    
    def update_track(self, camera_id: int, local_id: int, bev_pos: Tuple[float, float], 
                    pixel_pos: Tuple[int, int], class_name: str):
        """æ›´æ–°æœ¬åœ°è½¨è¿¹"""
        self.tracks[camera_id][local_id].append(bev_pos)
        self.pixel_tracks[camera_id][local_id].append(pixel_pos)
        self.track_classes[camera_id][local_id] = class_name
        
        if len(self.tracks[camera_id][local_id]) > self.max_history:
            self.tracks[camera_id][local_id].pop(0)
            self.pixel_tracks[camera_id][local_id].pop(0)
    
    def get_track_history(self, camera_id: int, local_id: int) -> List[Tuple[float, float]]:
        """è·å–è½¨è¿¹å†å²"""
        return self.tracks[camera_id].get(local_id, [])
    
    def get_pixel_track_history(self, camera_id: int, local_id: int) -> List[Tuple[int, int]]:
        """è·å–åƒç´ è½¨è¿¹å†å²"""
        return self.pixel_tracks[camera_id].get(local_id, [])
    
    def has_global_id(self, camera_id: int, local_id: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²åˆ†é…global_id"""
        return local_id in self.assigned_global_ids[camera_id]
    
    def get_global_id(self, camera_id: int, local_id: int) -> Optional[int]:
        """è·å–å·²åˆ†é…çš„global_id"""
        return self.assigned_global_ids[camera_id].get(local_id)
    
    def assign_global_id(self, camera_id: int, local_id: int, global_id: int):
        """è®°å½•local_idåˆ°global_idçš„æ˜ å°„"""
        self.assigned_global_ids[camera_id][local_id] = global_id
    
    def cleanup_track(self, camera_id: int, local_id: int):
        """æ¸…ç†è¿‡æœŸçš„è½¨è¿¹"""
        if local_id in self.tracks[camera_id]:
            del self.tracks[camera_id][local_id]
        if local_id in self.pixel_tracks[camera_id]:
            del self.pixel_tracks[camera_id][local_id]
        if local_id in self.assigned_global_ids[camera_id]:
            del self.assigned_global_ids[camera_id][local_id]
        if local_id in self.track_classes[camera_id]:
            del self.track_classes[camera_id][local_id]

def analyze_trajectory_for_global_assignment(pixel_track_history: List[Tuple[int, int]], 
                                            min_trajectory_length: int = 3,
                                            pixel_bottom_threshold: float = 700,
                                            pixel_top_threshold: float = 1080) -> bool:
    """
    åˆ†æè½¨è¿¹æ˜¯å¦å€¼å¾—åˆ†é…global_id
    åŸºäºåƒç´ Yå€¼åˆ¤æ–­æ˜¯å¦åœ¨åº•éƒ¨åŒºåŸŸ
    """
    if len(pixel_track_history) < min_trajectory_length:
        return False
    
    start_pos = pixel_track_history[0]
    start_y = start_pos[1]
    
    if pixel_bottom_threshold <= start_y <= pixel_top_threshold:
        return True
    
    return False

# --- å‡ ä½•å’Œæ£€æµ‹å·¥å…·ç±» ---
class GeometryUtils:
    @staticmethod
    def project_pixel_to_bev(H: np.ndarray, u: float, v: float) -> Optional[Tuple[float, float]]:
        """å•ä¸ªåƒç´ åˆ°BEVçš„è½¬æ¢ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰"""
        p = np.array([u, v, 1.0])
        q = H @ p
        if abs(q[2]) < 1e-8: return None
        x, y = q[0] / q[2], q[1] / q[2]
        if 0 <= x < Config.IMAGE_WIDTH and 0 <= y < Config.IMAGE_HEIGHT: return (x, y)
        return None


    @staticmethod
    def bev_to_geo(x_bev: float, y_bev: float) -> Optional[Tuple[float, float]]:
        """å•ä¸ªBEVåˆ°åœ°ç†åæ ‡çš„è½¬æ¢ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰"""
        try:
            p = np.array([x_bev, y_bev, 1.0])
            q = BEV_TO_GEO_MATRIX @ p
            q /= q[2]
            return q[0], q[1]
        except: return None


    @staticmethod
    def calculate_iou(box1: List[float], box2: List[float]) -> float:
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        x1_i, y1_i = max(x1_1, x1_2), max(y1_1, y1_2)
        x2_i, y2_i = min(x2_1, x2_2), min(y2_1, y2_2)
        if x2_i <= x1_i or y2_i <= y1_i: return 0.0
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        area1 = (x2_1 - x1_1) * (x2_1 - x1_1) 
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        return intersection / union if union > 0 else 0.0

    @staticmethod
    def is_in_public_area(bev_point: Tuple[float, float]) -> bool:
        return cv2.pointPolygonTest(PUBLIC_AREA_BEV, bev_point, False) >= 0

class DetectionUtils:
    @staticmethod
    def is_class_compatible(class1: str, class2: str) -> bool:
        if class1 == class2: return True
        return (class1 in Config.SIMILAR_CLASSES and 
                class2 in Config.SIMILAR_CLASSES[class1])

    @staticmethod
    def non_max_suppression(detections: List[dict], 
                          iou_threshold: float = Config.IOU_THRESHOLD) -> List[dict]:
        if not detections: return detections
        detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
        keep = []
        for det in detections:
            should_keep = True
            for kept_det in keep:
                if DetectionUtils.is_class_compatible(det['class'], kept_det['class']):
                    iou = GeometryUtils.calculate_iou(det['box'], kept_det['box'])
                    if iou > iou_threshold:
                        should_keep = False; break
            if should_keep: keep.append(det)
        return keep

    @staticmethod
    def is_turn_left(trajectory: List[Tuple[int, int]]) -> bool:
        if len(trajectory) < 2: return False
        start = np.array(trajectory[0])
        # ç®€åŒ–å·¦è½¬åˆ¤æ–­ï¼šå¦‚æœèµ·å§‹ç‚¹åœ¨å›¾åƒåº•éƒ¨ï¼Œåˆ™å¯èƒ½ä¸ºå·¦è½¬
        return start[1] >= Config.IMAGE_HEIGHT * 0.7 

# --- ç›®æ ‡ç¼“å†²åŒºç±» ---
class TargetBuffer:
    """ç›®æ ‡ç¼“å†²åŒºï¼Œç”¨äºæ—¶é—´çª—å£å†…çš„ç›®æ ‡åŒ¹é…"""
    def __init__(self, time_window: int = Config.TIME_WINDOW):
        self.buffer = deque(maxlen=500)
        self.time_window = time_window
        self.frame_counter = 0
        self.active_targets = {}

    def add_target(self, local_id: int, center_point: Tuple[float, float], 
                  class_name: str, confidence: float):
        target_info = {
            'local_id': local_id,
            'center_point': center_point,
            'class_name': class_name,
            'confidence': confidence,
            'timestamp': self.frame_counter
        }
        self.active_targets[local_id] = target_info
        self.buffer.append(target_info)
        self._cleanup_old_targets()

    def _cleanup_old_targets(self):
        cutoff_time = self.frame_counter - self.time_window
        self.buffer = deque([t for t in self.buffer if t['timestamp'] > cutoff_time], 
                          maxlen=self.buffer.maxlen)
        
        self.active_targets = {k: v for k, v in self.active_targets.items() 
                             if v['timestamp'] > cutoff_time}

    def find_matching_targets(self, class_name: str, 
                            tolerance_frames: int = Config.TOLERANCE_FRAMES) -> List[dict]:
        cutoff_time = self.frame_counter - tolerance_frames
        matches = [target for target in self.buffer 
                  if (target['timestamp'] >= cutoff_time and 
                      DetectionUtils.is_class_compatible(target['class_name'], class_name))]
        return sorted(matches, key=lambda x: x['timestamp'], reverse=True)

    def next_frame(self):
        self.frame_counter += 1

# --- å¹³æ»‘æ»¤æ³¢å™¨ç±» ---
class SmoothingFilter:
    """å¯¹ç›®æ ‡çš„BEVåæ ‡è¿›è¡Œå¹³æ»‘å¤„ç†"""
    def __init__(self, history_len: int = 10, alpha: float = 0.5):
        self.bev_history = defaultdict(lambda: deque(maxlen=history_len))
        self.alpha = alpha
        
    def _sliding_average(self, track_id: int, current_bev: Tuple[float, float]) -> Tuple[float, float]:
        """æ»‘åŠ¨å¹³å‡å¹³æ»‘"""
        self.bev_history[track_id].append(current_bev)
        
        history = self.bev_history[track_id]
        if len(history) < 2:
            return current_bev
            
        avg_x = sum(p[0] for p in history) / len(history)
        avg_y = sum(p[1] for p in history) / len(history)
        
        return (avg_x, avg_y)

    def _exponential_smoothing(self, track_id: int, current_bev: Tuple[float, float]) -> Tuple[float, float]:
        """æŒ‡æ•°å¹³æ»‘"""
        history_deque = self.bev_history[track_id]
        
        if not history_deque:
            smoothed_bev = current_bev
        else:
            last_smoothed_bev = history_deque[-1]
            smoothed_x = self.alpha * current_bev[0] + (1 - self.alpha) * last_smoothed_bev[0]
            smoothed_y = self.alpha * current_bev[1] + (1 - self.alpha) * last_smoothed_bev[1]
            smoothed_bev = (smoothed_x, smoothed_y)

        history_deque.append(smoothed_bev)
        return smoothed_bev
    
    def apply_smoothing(self, track_id: int, current_bev: Tuple[float, float], method: str = 'exponential') -> Tuple[float, float]:
        """åº”ç”¨å¹³æ»‘ç®—æ³•"""
        if method == 'sliding':
            return self._sliding_average(track_id, current_bev)
        else:
            return self._exponential_smoothing(track_id, current_bev)

    def remove_track(self, track_id: int):
        """ç§»é™¤ä¸å†æ´»è·ƒçš„ç›®æ ‡çš„å†å²è®°å½•"""
        self.bev_history.pop(track_id, None)

# --- è·¨æ‘„åƒå¤´èåˆç³»ç»Ÿ (æ–°èåˆç‰ˆ) ---
class CrossCameraFusion:
    """
    æ–°çš„è·¨æ‘„åƒå¤´èåˆç³»ç»Ÿ - åŸºäº0915èåˆé€»è¾‘
    æ ¸å¿ƒæ€è·¯ï¼š
    1. åŸºäºåƒç´ Yå€¼åˆ¤æ–­æ˜¯å¦åˆ†é…global_id
    2. ä½¿ç”¨èåˆåŒºè¿›å…¥æ—¶é—´åŒæ­¥è¿›è¡ŒåŒ¹é…
    3. é€šè¿‡local_to_globalæ°¸ä¹…ç»‘å®šé˜²æ­¢è·¨å¸§é‡å¤ç»‘å®š
    """
    
    def __init__(self):
        self.config = Config()
        
        # å…¨å±€ç›®æ ‡ç®¡ç†
        self.global_id_counter = 1
        self.global_targets: Dict[int, GlobalTarget] = {}  # global_id -> GlobalTarget
        self.local_to_global: Dict[Tuple[int, int], int] = {}  # (camera_id, local_id) -> global_id
        
        # æ–°å¢ï¼šæœ¬åœ°è½¨è¿¹ç¼“å†²åŒº
        self.local_track_buffer = LocalTrackBuffer(max_history=30)
        
        # é¢œè‰²ç®¡ç†
        self.colors: Dict[int, Tuple[int, int, int]] = {}
        
        # å¸§è®¡æ•°
        self.frame_count = 0
        self.json_output_data = []
        
        # ç¡®è®¤ç›®æ ‡ç®¡ç†
        self.confirmed_targets: Set[int] = set()
        self.target_frame_count: Dict[int, int] = defaultdict(int)
        
        # åˆå§‹åŒ–å¼‚æ­¥JSONä¿å­˜å™¨
        self.json_saver = AsyncJsonSaver(num_workers=1)
        
        print("âœ… æ–°èåˆç‰ˆCrossCameraFusionåˆå§‹åŒ–å®Œæˆ - åŸºäº0915èåˆé€»è¾‘")
    
    def _assign_color(self, global_id: int) -> Tuple[int, int, int]:
        """ä¸ºå…¨å±€IDåˆ†é…é¢œè‰²"""
        if global_id not in self.colors:
            np.random.seed(global_id)
            color = tuple(int(np.random.randint(0, 255)) for _ in range(3))
            self.colors[global_id] = color
        return self.colors[global_id]

    def assign_new_global_id(self, camera_id: int, local_id: int) -> int:
        """åˆ†é…æ–°çš„å…¨å±€ID"""
        global_id = self.global_id_counter
        self.global_id_counter += 1
        self._assign_color(global_id)
        return global_id

    def create_global_target(self, global_id: int, detection: dict, camera_id: int) -> GlobalTarget:
        """åˆ›å»ºå…¨å±€ç›®æ ‡"""
        center_x = int((detection['box'][0] + detection['box'][2]) / 2)
        center_y = int(detection['box'][3])
        
        # BEVåæ ‡è½¬æ¢
        H_matrix = CAMERA_MATRICES[camera_id]
        bev_result = GeometryUtils.project_pixel_to_bev(H_matrix, center_x, center_y)
        if not bev_result:
            bev_result = (0.0, 0.0)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨èåˆåŒºåŸŸ
        is_in_fusion_zone = GeometryUtils.is_in_public_area(bev_result)
        
        # å¦‚æœåœ¨èåˆåŒºï¼Œè®°å½•è¿›å…¥æ—¶é—´
        fusion_entry_frame = self.frame_count if is_in_fusion_zone else -1
        
        return GlobalTarget(
            global_id=global_id,
            camera_id=camera_id,
            local_id=detection['track_id'],
            class_name=detection['class'],
            bev_trajectory=[bev_result],
            pixel_trajectory=[(center_x, center_y)],
            last_seen_frame=self.frame_count,
            is_active=True,
            fusion_alpha=0.2,
            is_in_fusion_zone=is_in_fusion_zone,
            confidence_history=[detection['confidence']],
            fusion_entry_frame=fusion_entry_frame
        )
    
    def create_local_target(self, detection: dict, camera_id: int) -> LocalTarget:
        """åˆ›å»ºæœ¬åœ°ç›®æ ‡"""
        center_x = int((detection['box'][0] + detection['box'][2]) / 2)
        center_y = int(detection['box'][3])
            
        # BEVåæ ‡è½¬æ¢
        H_matrix = CAMERA_MATRICES[camera_id]
        bev_result = GeometryUtils.project_pixel_to_bev(H_matrix, center_x, center_y)
        if not bev_result:
            bev_result = (0.0, 0.0)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨èåˆåŒºåŸŸ
        is_in_fusion_area = GeometryUtils.is_in_public_area(bev_result)
        
        # å¦‚æœåœ¨èåˆåŒºï¼Œè®°å½•è¿›å…¥æ—¶é—´
        fusion_entry_frame = self.frame_count if is_in_fusion_area else -1
        
        return LocalTarget(
            local_id=detection['track_id'],
            camera_id=camera_id,
            class_name=detection['class'],
            current_bev_pos=bev_result,
            current_pixel_pos=(center_x, center_y),
            confidence=detection['confidence'],
            is_in_fusion_area=is_in_fusion_area,
            detection_box=detection['box'],
            fusion_entry_frame=fusion_entry_frame
        )
    
    def classify_targets(self, detections: List[dict], camera_id: int) -> Tuple[List[GlobalTarget], List[LocalTarget]]:
        """
        å°†æ£€æµ‹ç»“æœåˆ†ç±»ä¸ºglobal_targetså’Œlocal_targets
        åŸºäºè½¨è¿¹åˆ†æåˆ¤æ–­æ˜¯å¦åº”è¯¥å‡çº§ä¸ºglobal_id
        """
        global_targets = []
        local_targets = []
        
        for detection in detections:
            if 'track_id' not in detection:
                continue
            
            track_id = detection['track_id']
            class_name = detection['class']
            confidence = detection['confidence']
            center_y = detection['box'][3]  # åº•éƒ¨yåæ ‡
            
            # è·å– BEV åæ ‡
            H_matrix = CAMERA_MATRICES[camera_id]
            center_x = int((detection['box'][0] + detection['box'][2]) / 2)
            bev_result = GeometryUtils.project_pixel_to_bev(H_matrix, center_x, center_y)
            if not bev_result:
                bev_result = (0.0, 0.0)
            
            # æ›´æ–°æœ¬åœ°è½¨è¿¹ç¼“å†²åŒº
            self.local_track_buffer.update_track(camera_id, track_id, bev_result, (center_x, int(center_y)), class_name)
            track_history = self.local_track_buffer.get_track_history(camera_id, track_id)
            pixel_track_history = self.local_track_buffer.get_pixel_track_history(camera_id, track_id)
            
            # æ£€æŸ¥æ˜¯å¦å·²åˆ†é… global_id
            if self.local_track_buffer.has_global_id(camera_id, track_id):
                # å·²åˆ†é…è¿‡ï¼Œç›´æ¥æ›´æ–°ç°æœ‰çš„å…¨å±€ç›®æ ‡
                global_id = self.local_track_buffer.get_global_id(camera_id, track_id)
                global_target = self.global_targets.get(global_id)
                
                if global_target:
                    # æ›´æ–°å…¨å±€ç›®æ ‡çš„è½¨è¿¹
                    global_target.bev_trajectory.append(bev_result)
                    global_target.pixel_trajectory.append((center_x, int(center_y)))
                    global_target.confidence_history.append(confidence)
                    global_target.last_seen_frame = self.frame_count
                    
                    # æ›´æ–°èåˆåŒºè¿›å…¥æ—¶é—´
                    if global_target.is_in_fusion_zone and global_target.fusion_entry_frame == -1:
                        global_target.fusion_entry_frame = self.frame_count
                    
                    # æ›´æ–°èåˆåŒºçŠ¶æ€
                    current_bev = global_target.bev_trajectory[-1]
                    global_target.is_in_fusion_zone = GeometryUtils.is_in_public_area(current_bev)
                    
                    # é™åˆ¶è½¨è¿¹é•¿åº¦
                    max_length = 50
                    if len(global_target.bev_trajectory) > max_length:
                        global_target.bev_trajectory = global_target.bev_trajectory[-max_length:]
                        global_target.pixel_trajectory = global_target.pixel_trajectory[-max_length:]
                        global_target.confidence_history = global_target.confidence_history[-max_length:]
                    
                    global_targets.append(global_target)
                continue
            
            # æœªåˆ†é…è¿‡ï¼ŒåŸºäºè½¨è¿¹åˆ†æåˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ†é… global_id
            if analyze_trajectory_for_global_assignment(pixel_track_history, 
                                                       min_trajectory_length=3,
                                                       pixel_bottom_threshold=self.config.PIXEL_BOTTOM_THRESHOLD,
                                                       pixel_top_threshold=self.config.PIXEL_TOP_THRESHOLD):
                # æ»¡è¶³æ¡ä»¶ï¼Œåˆ†é…æ–°çš„ global_id
                global_id = self.assign_new_global_id(camera_id, track_id)
                global_target = self.create_global_target(global_id, detection, camera_id)
                
                # è®°å½•åˆ†é…å…³ç³»
                self.local_track_buffer.assign_global_id(camera_id, track_id, global_id)
                
                # æ·»åŠ åˆ°å…¨å±€ç›®æ ‡å­—å…¸
                self.global_targets[global_id] = global_target
                self.target_frame_count[global_id] = 1
                
                global_targets.append(global_target)
            else:
                # ä¸æ»¡è¶³æ¡ä»¶ï¼Œä½œä¸ºæœ¬åœ°ç›®æ ‡
                local_target = self.create_local_target(detection, camera_id)
                local_targets.append(local_target)
        
        return global_targets, local_targets
    
    def _smoothly_merge_trajectory(self, global_target: GlobalTarget, 
                                  local_target: LocalTarget):
        """åŠ¨æ€åŠ æƒå¹³æ»‘è½¨è¿¹èåˆ"""
        # è®¡ç®—åŠ æƒä½ç½®
        current_bev = global_target.bev_trajectory[-1]
        local_bev = local_target.current_bev_pos
        
        # ä½¿ç”¨åŠ¨æ€èåˆæƒé‡
        alpha = global_target.fusion_alpha
        new_bev_x = ((1.0 - alpha) * current_bev[0] + alpha * local_bev[0])
        new_bev_y = ((1.0 - alpha) * current_bev[1] + alpha * local_bev[1])
        
        # æ›´æ–°è½¨è¿¹
        global_target.bev_trajectory.append((new_bev_x, new_bev_y))
        global_target.pixel_trajectory.append(local_target.current_pixel_pos)
        global_target.confidence_history.append(local_target.confidence)
        global_target.last_seen_frame = self.frame_count
        
        # åŠ¨æ€å¢åŠ èåˆæƒé‡ï¼Œå®ç°å¹³æ»‘è¿‡æ¸¡
        global_target.fusion_alpha += 0.01
        if global_target.fusion_alpha > 1.0:
            global_target.fusion_alpha = 1.0
        
        # æ›´æ–°èåˆåŒºåŸŸçŠ¶æ€
        global_target.is_in_fusion_zone = local_target.is_in_fusion_area
    
    def _perform_matching(self, local_targets_this_frame: List[LocalTarget], 
                     active_global_targets: List[GlobalTarget]):
        """
        æ ¸å¿ƒåŒ¹é…æ–¹æ³•ï¼šåŸºäºèåˆåŒºè¿›å…¥æ—¶é—´è¿›è¡Œæ—¶é—´åŒæ­¥åŒ¹é…
        ä½¿ç”¨æ°¸ä¹…ç»‘å®šè®°å½•é˜²æ­¢è·¨å¸§é‡å¤ç»‘å®š
        """
        # ç”¨äºé”å®šæœ¬å¸§å·²åŒ¹é…çš„ global_idï¼Œé˜²æ­¢ä¸€å¯¹å¤š
        locked_global_ids_this_frame = set()

        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å·²è¢«æ°¸ä¹…ç»‘å®šçš„ global_id çš„é›†åˆ
        permanently_bound_global_ids = set(self.local_to_global.values())

        # 1. é¢„å¤„ç†ï¼šåˆ·æ–°æ‰€æœ‰å…¨å±€ç›®æ ‡çš„èåˆåŒºçŠ¶æ€å’Œè¿›å…¥æ—¶é—´
        for gt in active_global_targets:
            if gt.bev_trajectory:
                current_bev = gt.bev_trajectory[-1]
                is_now_in_zone = GeometryUtils.is_in_public_area(current_bev)
                gt.is_in_fusion_zone = is_now_in_zone
                if is_now_in_zone and gt.fusion_entry_frame == -1:
                    gt.fusion_entry_frame = self.frame_count

        time_window = self.config.FUSION_TIME_WINDOW

        # 2. éå†æ‰€æœ‰æœ¬å¸§çš„ local_target è¿›è¡Œå¤„ç†
        for local_target in local_targets_this_frame:
            lookup_key = (local_target.camera_id, local_target.local_id)

            # 2.1 æ£€æŸ¥æ­¤ local_target æ˜¯å¦å·²ç»ç»‘å®š
            if lookup_key in self.local_to_global:
                bound_global_id = self.local_to_global[lookup_key]
                bound_global_target = self.global_targets.get(bound_global_id)

                if bound_global_target:
                    self._smoothly_merge_trajectory(bound_global_target, local_target)
                    local_target.matched_global_id = bound_global_id
                else:
                    del self.local_to_global[lookup_key]
                
                continue

            # 2.2 å¦‚æœæœªç»‘å®šï¼Œæ‰§è¡Œé¦–æ¬¡åŒ¹é…é€»è¾‘
            if not local_target.is_in_fusion_area:
                continue
                
            if local_target.fusion_entry_frame == -1:
                local_target.fusion_entry_frame = self.frame_count
            
            # ç¡®å®šå€™é€‰æ±  - åŸºäºæ‘„åƒå¤´é…å¯¹å…³ç³»ï¼ˆ0915æ–¹å¼ï¼‰
            candidate_globals = []
            if local_target.camera_id == 1:
                candidate_globals = [gt for gt in active_global_targets if gt.camera_id == 2]
            elif local_target.camera_id == 2:
                candidate_globals = [gt for gt in active_global_targets if gt.camera_id in [1, 3]]
            elif local_target.camera_id == 3:
                candidate_globals = [gt for gt in active_global_targets if gt.camera_id == 2]
            
            # ç­›é€‰æ—¶ï¼ŒåŒæ—¶æ£€æŸ¥å¸§å†…é”å’Œæ°¸ä¹…ç»‘å®šçŠ¶æ€
            fusion_candidates = [
                gt for gt in candidate_globals 
                if (gt.is_in_fusion_zone and 
                    gt.global_id not in locked_global_ids_this_frame and
                    gt.global_id not in permanently_bound_global_ids)
            ]
            
            if not fusion_candidates:
                continue
            
            best_match = None
            best_time_diff = float('inf')
            
            for candidate in fusion_candidates:
                if not DetectionUtils.is_class_compatible(local_target.class_name, candidate.class_name):
                    continue
                if candidate.fusion_entry_frame == -1:
                    continue
                time_diff = abs(local_target.fusion_entry_frame - candidate.fusion_entry_frame)
                if time_diff <= time_window and time_diff < best_time_diff:
                    best_time_diff = time_diff
                    best_match = candidate
            
            if best_match:
                local_target.matched_global_id = best_match.global_id
                self.local_to_global[lookup_key] = best_match.global_id
                locked_global_ids_this_frame.add(best_match.global_id)
                self._smoothly_merge_trajectory(best_match, local_target)
    
    def update_global_state(self, all_global_targets: List[GlobalTarget], all_local_targets: List[LocalTarget]):
        """æ›´æ–°å…¨å±€çŠ¶æ€"""
        # å¤„ç†ç›´æ¥çš„å…¨å±€ç›®æ ‡
        for global_target in all_global_targets:
            self.target_frame_count[global_target.global_id] += 1
            
            # æ›´æ–°èåˆåŒºåŸŸçŠ¶æ€
            if global_target.bev_trajectory:
                current_bev = global_target.bev_trajectory[-1]
                global_target.is_in_fusion_zone = GeometryUtils.is_in_public_area(current_bev)
            
            # ç¡®è®¤ç›®æ ‡
            if (self.target_frame_count[global_target.global_id] >= self.config.MIN_FRAMES_THRESHOLD and 
                global_target.global_id not in self.confirmed_targets):
                self.confirmed_targets.add(global_target.global_id)
    
    def process_detections(self, detections: List[dict], camera_id: int, perf_monitor=None) -> Tuple[List[GlobalTarget], List[LocalTarget]]:
        """å¤„ç†å•ä¸ªæ‘„åƒå¤´çš„æ£€æµ‹ç»“æœ"""
        if perf_monitor:
            perf_monitor.start_timer('process_detections')
        
        # åˆ†ç±»ç›®æ ‡
        global_targets, local_targets = self.classify_targets(detections, camera_id)
        
        # å°†å…¨å±€ç›®æ ‡æ·»åŠ åˆ°å…¨å±€ç›®æ ‡å­—å…¸
        for global_target in global_targets:
            if global_target.global_id not in self.global_targets:
                self.global_targets[global_target.global_id] = global_target
                self.target_frame_count[global_target.global_id] = 1
        
        if perf_monitor:
            duration = perf_monitor.end_timer('process_detections')
            perf_monitor.record_fusion_stats('process_detections', duration, {
                'detection_count': len(detections),
                'global_target_count': len(global_targets),
                'local_target_count': len(local_targets)
            })
        
        return global_targets, local_targets
    
    def generate_json_data_new(self, all_global_targets: List[GlobalTarget], 
                              all_local_targets: List[LocalTarget]) -> dict:
        """ç”Ÿæˆæ–°çš„JSONæ•°æ®"""
        current_time_ms = int(self.frame_count * 1000 / self.config.FPS)
        participants = []
        
        # å¤„ç†å…¨å±€ç›®æ ‡
        for global_target in all_global_targets:
            if not self.is_confirmed_target(global_target.global_id):
                continue
            
            if not global_target.bev_trajectory:
                continue
            
            current_bev = global_target.bev_trajectory[-1]
            geo_result = GeometryUtils.bev_to_geo(current_bev[0], current_bev[1])
            if not geo_result:
                continue
            
            lng, lat = geo_result
            participants.append({
                "pid": global_target.global_id,
                "type": global_target.class_name,
                "plate": f"GID{global_target.global_id}",
                "heading": 0,
                "lon": lng,
                "lat": lat
            })
        
        # å¤„ç†å·²åŒ¹é…çš„æœ¬åœ°ç›®æ ‡
        for local_target in all_local_targets:
            if not local_target.matched_global_id:
                continue
                
            if not self.is_confirmed_target(local_target.matched_global_id):
                continue
                
            geo_result = GeometryUtils.bev_to_geo(local_target.current_bev_pos[0], local_target.current_bev_pos[1])
            if not geo_result:
                continue
                
            lng, lat = geo_result
            participants.append({
                "pid": local_target.matched_global_id,
                "type": local_target.class_name,
                "plate": f"GID{local_target.matched_global_id}",
                "heading": 0,
                "lon": lat,
                "lat": lng
            })
        
        return {
            "reportTime": current_time_ms,
            "participant": participants
        }

    def is_confirmed_target(self, global_id: int) -> bool:
        """æ£€æŸ¥ç›®æ ‡æ˜¯å¦å·²ç¡®è®¤"""
        return global_id in self.confirmed_targets

    def cleanup_inactive_targets(self):
        """æ¸…ç†ä¸æ´»è·ƒç›®æ ‡"""
        if self.frame_count % 20 != 0:
            return
        
        inactive_threshold = 100
        current_time = self.frame_count
        
        # æ¸…ç†ä¸æ´»è·ƒçš„å…¨å±€ç›®æ ‡
        inactive_global_ids = []
        for global_id, global_target in self.global_targets.items():
            if current_time - global_target.last_seen_frame > inactive_threshold:
                inactive_global_ids.append(global_id)
        
        for global_id in inactive_global_ids:
            self.global_targets.pop(global_id, None)
            self.colors.pop(global_id, None)
            self.target_frame_count.pop(global_id, None)
            self.confirmed_targets.discard(global_id)
            
            # æ¸…ç†æ˜ å°„å…³ç³»
            keys_to_remove = [k for k, v in self.local_to_global.items() if v == global_id]
            for key in keys_to_remove:
                del self.local_to_global[key]

    def next_frame(self):
        """è¿›å…¥ä¸‹ä¸€å¸§"""
        self.frame_count += 1
        self.cleanup_inactive_targets()

    def save_json_data(self, output_file: str):
        """ä¿å­˜JSONæ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.json_output_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… JSONæ•°æ®å·²ä¿å­˜: {output_file}, å…±{len(self.json_output_data)}å¸§")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å‡ºé”™: {e}")
    
    def save_json_data_realtime(self, output_file: str, current_frame: int):
        """å¼‚æ­¥å®æ—¶ä¿å­˜JSONæ•°æ®"""
        try:
            realtime_data = {
                "frame_number": current_frame,
                "timestamp": time.time(),
                "total_frames": len(self.json_output_data),
                "global_targets": len(self.global_targets),
                "confirmed_targets": len(self.confirmed_targets),
                "data": self.json_output_data
            }
            
            # ä½¿ç”¨å¼‚æ­¥ä¿å­˜å™¨éé˜»å¡åœ°ä¿å­˜JSON
            self.json_saver.save_async(output_file, realtime_data)
            
            if current_frame % 10 == 0:
                print(f"ğŸ’¾ å¼‚æ­¥JSONæäº¤: Frame {current_frame}, Global={len(self.global_targets)}, Confirmed={len(self.confirmed_targets)}")
                
        except Exception as e:
            print(f"âŒ å¼‚æ­¥JSONæäº¤å¤±è´¥: {e}")

# --- æ€§èƒ½ç›‘è§†å™¨ç±» ---
class PerformanceMonitor:
    """æ€§èƒ½ç›‘è§†å™¨ï¼Œç”¨äºå®šä½ç³»ç»Ÿç“¶é¢ˆ"""
    
    def __init__(self):
        self.timers = {}
        self.counters = {}
        self.queue_stats = {}
        self.fusion_stats = {}
        self.last_report_time = time.time()
        self.report_interval = 10.0
        
        self.counters = {
            'frames_processed': 0,
            'frames_synchronized': 0,
            'detections_processed': 0,
            'fusion_operations': 0,
            'bev_conversions': 0,
            'tracker_updates': 0,
            'queue_operations': 0,
            'mqtt_sends': 0,
            'mqtt_failures': 0
        }
        
        print("ğŸ“Š æ€§èƒ½ç›‘è§†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def start_timer(self, name: str):
        self.timers[name] = time.time()
    
    def end_timer(self, name: str) -> float:
        if name not in self.timers:
            return 0.0
        elapsed = (time.time() - self.timers[name]) * 1000
        del self.timers[name]
        return elapsed
    
    def add_counter(self, name: str, value: int = 1):
        if name in self.counters:
            self.counters[name] += value
        else:
            self.counters[name] = value
    
    def record_queue_stats(self, camera_id: int, queue_size: int, operation: str):
        if camera_id not in self.queue_stats:
            self.queue_stats[camera_id] = {
                'max_size': 0,
                'avg_size': 0,
                'operations': 0,
                'total_size': 0
            }
        
        stats = self.queue_stats[camera_id]
        stats['max_size'] = max(stats['max_size'], queue_size)
        stats['operations'] += 1
        stats['total_size'] += queue_size
        stats['avg_size'] = stats['total_size'] / stats['operations']
    
    def record_fusion_stats(self, operation: str, duration_ms: float, details: dict = None):
        if operation not in self.fusion_stats:
            self.fusion_stats[operation] = {
                'count': 0,
                'total_time': 0.0,
                'max_time': 0.0,
                'min_time': float('inf'),
                'avg_time': 0.0
            }
        
        stats = self.fusion_stats[operation]
        stats['count'] += 1
        stats['total_time'] += duration_ms
        stats['max_time'] = max(stats['max_time'], duration_ms)
        stats['min_time'] = min(stats['min_time'], duration_ms)
        stats['avg_time'] = stats['total_time'] / stats['count']
    
    def get_performance_report(self) -> str:
        current_time = time.time()
        elapsed = current_time - self.last_report_time
        
        if elapsed < self.report_interval:
            return ""
        
        self.last_report_time = current_time
        
        report = []
        report.append("\n" + "="*60)
        report.append("ğŸ“Š æ€§èƒ½ç›‘æ§æŠ¥å‘Š")
        report.append("="*60)
        
        fps = self.counters['frames_processed'] / elapsed if elapsed > 0 else 0
        sync_fps = self.counters['frames_synchronized'] / elapsed if elapsed > 0 else 0
        report.append(f"ğŸš€ å¤„ç†é€Ÿåº¦:")
        report.append(f"  æ€»å¸§å¤„ç†é€Ÿåº¦: {fps:.2f} FPS")
        report.append(f"  åŒæ­¥å¸§å¤„ç†é€Ÿåº¦: {sync_fps:.2f} FPS")
        
        if self.fusion_stats:
            report.append(f"\nğŸ”„ èåˆç®—æ³•æ€§èƒ½:")
            for operation, stats in self.fusion_stats.items():
                report.append(f"  {operation}: å¹³å‡{stats['avg_time']:.2f}ms")
        
        report.append("="*60)
        
        return "\n".join(report)
    
    def reset_counters(self):
        for key in self.counters:
            self.counters[key] = 0
        self.queue_stats.clear()
        self.fusion_stats.clear()

# --- SDK æ¨ç†ç±» (ç”Ÿäº§è€…ï¼Œç²¾ç®€ç‰ˆ) ---
class yolov5_SDK(infer_process):
    """ç²¾ç®€ç‰ˆçš„ SDK æ¨ç†è¿›ç¨‹"""
    def __init__(self, attr, result_queue):
        super().__init__(attr)
        self.class_num = self.model_desc.outputTensorAttrArray[0].dims[3] - 5
        self.anchor_size = self.model_desc.outputTensorAttrArray[0].dims[1]
        
        self.result_queue = result_queue
        self.frame_count = 0
        
        self.boxes_info, ret = sdk.lyn_malloc(ctypes.sizeof(Box))
        if ret != 0:
            raise RuntimeError(f"Camera{self.attr.chan_id + 1}: å†…å­˜åˆ†é…å¤±è´¥: {ret}")

    def update_class_name(self, class_name_path: str) -> None:
        try:
            with open(class_name_path, 'r') as file:
                file_content = file.read()
                pattern = re.compile(r"^(x7|normal):([^,]+(,[^,]+)*)$", re.IGNORECASE)
                if not pattern.match(file_content):
                    print(f'"{file_content}" is not right!')
                    os._exit(-1)
        except FileNotFoundError:
            print(f"File at path '{class_name_path}' not found.")
            return None
        except IOError as e:
            print(f"Error reading file at path '{class_name_path}': {e}")
            return None
        
        ary = np.fromfile(class_name_path)
        ptr = sdk.lyn_numpy_to_ptr(ary)
        device_ptr, ret = sdk.lyn_malloc(ary.nbytes)
        sdk.lyn_memcpy(device_ptr, ptr, ary.nbytes, sdk.ClientToServer)
        class_name_arg = struct.pack("Pi", pythonapi.PyCapsule_GetPointer(device_ptr, None), ary.nbytes)
        sdk.lyn_plugin_run_async(self.ipe_stream, self.plugin, "lynClassNameUpdata", class_name_arg, len(class_name_arg))
        sdk.lyn_synchronize_stream(self.ipe_stream)
        sdk.lyn_free(device_ptr)
        print(f"âœ… Camera{self.attr.chan_id + 1} æˆåŠŸæ›´æ–°ç±»åˆ«åç§°")

    def process_box_data_callback(self, params):
        try:
            boxes_info = params[0]
            frame_count = params[1]
            
            dst_img_size = ctypes.sizeof(Box)
            host_buf_arr = np.ones(dst_img_size, dtype=np.uint8)
            host_buf = sdk.lyn_numpy_to_ptr(host_buf_arr)
            ret = sdk.lyn_memcpy(
                host_buf, boxes_info, dst_img_size, 
                sdk.lyn_memcpy_dir_t.ServerToClient
            )
            if ret != 0:
                print(f"âŒ Camera{self.attr.chan_id + 1} memcpyå¤±è´¥: {ret}", flush=True)
                return 0
            
            pythonapi.PyCapsule_GetPointer.restype = c_void_p
            pythonapi.PyCapsule_GetPointer.argtypes = [py_object, c_char_p]
            host_buf_c = pythonapi.PyCapsule_GetPointer(host_buf, None)
            box_data = ctypes.cast(host_buf_c, ctypes.POINTER(Box)).contents
            
            frame_result = self.extract_detection_info_from_box(box_data, frame_count)
            
            if frame_result:
                self.result_queue.put(copy.deepcopy(frame_result))
                if frame_count % 30 == 0:
                    print(f"C{self.attr.chan_id + 1} | F{frame_count}: PUSH {frame_result['boxes_num']} boxes", flush=True)
            else:
                self.result_queue.put({
                    'frame_id': frame_count, 'camera_id': self.attr.chan_id + 1,
                    'boxes_num': 0, 'detections': []
                })
            
            return 0
        except Exception as e:
            print(f"âŒ Camera{self.attr.chan_id + 1} callbacké”™è¯¯: {e}", flush=True)
            import traceback
            traceback.print_exc()
            return 0

    def extract_detection_info_from_box(self, box_data, frame_count):
        if not box_data: return None
            
        frame_result = {
            'frame_id': frame_count,
            'camera_id': self.attr.chan_id + 1,
            'boxes_num': box_data.boxesnum,
            'detections': []
        }
        
        for i in range(box_data.boxesnum):
            try:
                box = box_data.boxes[i]
            except Exception as e:
                print(f"âŒ Camera{self.attr.chan_id + 1} è®¿é—®boxes[{i}]é”™è¯¯: {e}", flush=True)
                continue
                
            try:
                if hasattr(box, 'label') and box.label is not None:
                    if isinstance(box.label, str):
                        label_str = box.label
                        try:
                            class_id = int(box.label)
                        except ValueError:
                            class_id = hash(box.label) % 1000
                    else:
                        class_id = int(box.label)
                        label_str = NAMES[class_id] if class_id < len(NAMES) else f"class_{class_id}"
                else:
                    label_str = "unknown"
                    class_id = 0
            except (ValueError, UnicodeError, TypeError) as e:
                label_str = "unknown"
                class_id = 0
            
            detection = {
                'box': [float(box.xmin), float(box.ymin), float(box.xmax), float(box.ymax)],
                'confidence': float(box.score),
                'class': label_str, 
            }
            frame_result['detections'].append(detection)
        
        return frame_result

    def plugin_process(self, apu_output_data, cb_data):
        if self.frame_count == 0 or self.frame_count % 30 == 0:
            print(f"ğŸ¯ Camera{self.attr.chan_id + 1} plugin_process è¢«è°ƒç”¨ï¼Œå¸§{self.frame_count}", flush=True)
        
        try:
            ret = sdk.lyn_record_event(self.apu_stream, self.apu_event)
            if ret != 0: 
                print(f"âŒ Camera{self.attr.chan_id + 1} lyn_record_event å¤±è´¥: {ret}", flush=True)
                return
            ret = sdk.lyn_stream_wait_event(self.plugin_stream, self.apu_event)
            if ret != 0: 
                print(f"âŒ Camera{self.attr.chan_id + 1} lyn_stream_wait_event å¤±è´¥: {ret}", flush=True)
                return
            
            pythonapi.PyCapsule_GetPointer.restype = c_void_p
            pythonapi.PyCapsule_GetPointer.argtypes = [py_object, c_char_p]
            apu_data_ptr = pythonapi.PyCapsule_GetPointer(apu_output_data, None)
            boxes_info_ptr = pythonapi.PyCapsule_GetPointer(self.boxes_info, None)

            post_para = struct.pack(
                '6IH2f?2P',
                self.codec_para.width, self.codec_para.height, self.model_width,
                self.model_height, self.class_num, 500, self.anchor_size,
                0.25, 0.45, True, apu_data_ptr, boxes_info_ptr,
            )
            ret = sdk.lyn_plugin_run_async(
                self.plugin_stream, self.plugin, "lynPostProcess", post_para, len(post_para)
            )
            if ret != 0:
                print(f"âŒ Camera{self.attr.chan_id + 1} lyn_plugin_run_async å¤±è´¥: {ret}", flush=True)
                return
            
            ret = sdk.lyn_stream_add_callback(
                self.plugin_stream,
                self.process_box_data_callback,
                [self.boxes_info, self.frame_count],
            )
            if ret != 0:
                print(f"âŒ Camera{self.attr.chan_id + 1} lyn_stream_add_callback å¤±è´¥: {ret}", flush=True)
                return
            
            self.frame_count += 1

            ret = sdk.lyn_stream_add_async_callback(
                self.plugin_stream, free_to_pool_callback, [self.apu_output_mem_pool, apu_output_data]
            )
            
        except Exception as e:
            print(f"âŒ Camera{self.attr.chan_id + 1}: plugin_process å‡ºé”™: {e}", flush=True)
            import traceback
            traceback.print_exc()

    def run(self, cancel_flag):
        super().run(cancel_flag)

# --- è¾…åŠ©å‡½æ•° ---
def cancel_process(signum, frame):
    global cancel_flag
    cancel_flag.value = True
    print("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")

def create_sdk_worker_process(camera_id: int, video_path: str, result_queue: multiprocessing.Queue):
    """åˆ›å»ºå¹¶è¿è¡Œä¸€ä¸ªç‹¬ç«‹çš„ SDK æ¨ç†å­è¿›ç¨‹"""
    try:
        print(f"ğŸ”§ Camera{camera_id} å¼€å§‹åˆå§‹åŒ–SDK...", flush=True)
        
        attr = infer_process_attr()
        attr.url = video_path
        attr.device_id = 0
        attr.chan_id = camera_id - 1
        attr.plugin_path = "/usr/local/lynxi/sdk/sdk-samples/plugin/obj/libYolov5Plugin.so"
        attr.model_path = "/root/yolov5-7.0_lyngor1.17.0/best_yolov5s_onnx/Net_0/"
        attr.show_type = 2
        attr.output_path = ""
        
        worker = yolov5_SDK(attr, result_queue) 
        
        class_name_path = "/usr/local/lynxi/sdk/sdk-samples/data/class.txt"
        if os.path.exists(class_name_path):
            worker.update_class_name(class_name_path)
        
        print(f"ğŸš€ Camera{camera_id} å¼€å§‹è¿è¡ŒSDKæ¨ç†...", flush=True)
        worker.run(cancel_flag)
        
        while not cancel_flag.value:
            time.sleep(1)
        
        print(f"ğŸ›‘ Camera{camera_id} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...", flush=True)
        worker.close()
    except Exception as e:
        print(f"âŒ Camera{camera_id} SDKè¿›ç¨‹å¤±è´¥: {e}", flush=True)
        import traceback
        traceback.print_exc()
        os._exit(1)

def filter_by_detect_areas(detections: List[dict], areas: List[np.ndarray]) -> List[dict]:
    """æ ¹æ®æ£€æµ‹åŒºåŸŸè¿‡æ»¤æ£€æµ‹ç»“æœ"""
    filtered_detections = []
    for detection in detections:
        x1, y1, x2, y2 = detection['box']
        center_x, center_y = int((x1 + x2) / 2), int(y2) 
        in_detect_area = any(cv2.pointPolygonTest(area, (center_x, center_y), False) >= 0 
                           for area in areas)
        if in_detect_area:
            filtered_detections.append(detection)
    return filtered_detections

def batch_prepare_tracker_input(nms_detections: List[dict]) -> np.ndarray:
    """æ‰¹é‡å‡†å¤‡è·Ÿè¸ªå™¨è¾“å…¥"""
    if not nms_detections:
        return np.empty((0, 6), dtype=np.float32)
    
    boxes_scores = np.array([[d['box'][0], d['box'][1], d['box'][2], d['box'][3], d['confidence']] for d in nms_detections])
    labels = np.array([NAMES.index(d['class']) if d['class'] in NAMES else 0 for d in nms_detections])
    
    tracker_input_array = np.column_stack([boxes_scores, labels]).astype(np.float32)
    return tracker_input_array

def batch_convert_track_results(tracked_objects: List, result: dict, camera_id: int, current_frame: int, 
                               original_detections: List[dict] = None) -> List[dict]:
    """æ‰¹é‡è½¬æ¢è·Ÿè¸ªç»“æœ"""
    tracked_detections = []
    
    for track in tracked_objects:
        tlwh = track.tlwh
        tlbr = [tlwh[0], tlwh[1], tlwh[0] + tlwh[2], tlwh[1] + tlwh[3]]
        
        class_name = 'vehicle'
        if original_detections:
            best_iou = 0
            for orig_det in original_detections:
                iou = GeometryUtils.calculate_iou(tlbr, orig_det['box'])
                if iou > best_iou and iou > 0.3:
                    best_iou = iou
                    class_name = orig_det['class']
        
        detection = {
            'box': tlbr,
            'confidence': track.score,
            'class': class_name,
            'track_id': track.track_id,
            'local_id': track.track_id,
            'center_point': [(tlbr[0] + tlbr[2]) / 2, (tlbr[1] + tlbr[3]) / 2],
            'timestamp': result.get('timestamp', time.time()),
            'frame_number': result.get('frame_number', current_frame),
            'camera_id': camera_id,
            'sync_id': result.get('sync_id', f"C{camera_id}_F{current_frame}")
        }
        tracked_detections.append(detection)
    
    return tracked_detections

# --- æ–°å¢ï¼šé€Ÿåº¦ç›‘æ§ç±» ---
class SpeedMonitor:
    """å®æ—¶é€Ÿåº¦ç›‘æ§ - è¿½è¸ªæ¯ä¸ªæ‘„åƒå¤´çš„å¸§åˆ°è¾¾é€Ÿåº¦å’Œå¤„ç†é€Ÿåº¦"""
    
    def __init__(self, window_size=300):
        self.window_size = window_size
        
        # å¸§åˆ°è¾¾æ—¶é—´è®°å½•ï¼ˆç”¨äºè®¡ç®—å®é™…FPSï¼‰
        self.frame_arrival_times = {1: deque(maxlen=window_size), 
                                   2: deque(maxlen=window_size), 
                                   3: deque(maxlen=window_size)}
        self.last_frame_id = {1: -1, 2: -1, 3: -1}
        
        # å¤„ç†ç¯èŠ‚ç»Ÿè®¡
        self.detection_times = {1: deque(maxlen=100), 
                               2: deque(maxlen=100), 
                               3: deque(maxlen=100)}
        self.tracking_times = {1: deque(maxlen=100), 
                              2: deque(maxlen=100), 
                              3: deque(maxlen=100)}
        self.fusion_times = deque(maxlen=100)
        
        # ç¼“å†²åŒºç›‘æ§
        self.buffer_snapshots = {1: deque(maxlen=100), 
                                2: deque(maxlen=100), 
                                3: deque(maxlen=100)}
        
        # æŠ¥å‘Šæ—¶é—´
        self.last_report_time = time.time()
        self.report_interval = 10.0
        
        print("ğŸ“Š é€Ÿåº¦ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def record_frame_arrival(self, camera_id, frame_id):
        """è®°å½•å¸§åˆ°è¾¾äº‹ä»¶"""
        now = time.time()
        self.frame_arrival_times[camera_id].append((frame_id, now))
        self.last_frame_id[camera_id] = frame_id
    
    def record_detection_time(self, camera_id, duration_ms):
        """è®°å½•æ£€æµ‹è€—æ—¶"""
        self.detection_times[camera_id].append(duration_ms)
    
    def record_tracking_time(self, camera_id, duration_ms):
        """è®°å½•è·Ÿè¸ªè€—æ—¶"""
        self.tracking_times[camera_id].append(duration_ms)
    
    def record_fusion_time(self, duration_ms):
        """è®°å½•èåˆè€—æ—¶"""
        self.fusion_times.append(duration_ms)
    
    def record_buffer_snapshot(self, camera_id, buffer_size):
        """è®°å½•ç¼“å†²åŒºå¤§å°"""
        self.buffer_snapshots[camera_id].append((time.time(), buffer_size))
    
    def get_real_fps(self, camera_id):
        """è®¡ç®—å®é™…FPSï¼ˆæœ€è¿‘100å¸§ï¼‰"""
        times = self.frame_arrival_times[camera_id]
        if len(times) < 2:
            return 0
        
        frame_count = times[-1][0] - times[0][0]
        time_diff = times[-1][1] - times[0][1]
        
        if time_diff > 0:
            return frame_count / time_diff
        return 0
    
    def get_buffer_growth_rate(self, camera_id):
        """è®¡ç®—ç¼“å†²åŒºå¢é•¿é€Ÿç‡ï¼ˆå¸§/ç§’ï¼‰"""
        snapshots = self.buffer_snapshots[camera_id]
        if len(snapshots) < 2:
            return 0
        
        size_diff = snapshots[-1][1] - snapshots[0][1]
        time_diff = snapshots[-1][0] - snapshots[0][0]
        
        if time_diff > 0:
            return size_diff / time_diff
        return 0
    
    def get_speed_statistics(self):
        """ç”Ÿæˆè¯¦ç»†çš„é€Ÿåº¦ç»Ÿè®¡æŠ¥å‘Š"""
        report = {}
        report['timestamp'] = time.time()
        report['cameras'] = {}
        report['processing_stages'] = {}
        
        # 1. æ‘„åƒå¤´é€Ÿåº¦ç»Ÿè®¡
        fps_values = []
        for cam_id in [1, 2, 3]:
            real_fps = self.get_real_fps(cam_id)
            buffer_growth = self.get_buffer_growth_rate(cam_id)
            fps_values.append(real_fps)
            
            report['cameras'][f'C{cam_id}'] = {
                'real_fps': round(real_fps, 2),
                'buffer_growth_rate': round(buffer_growth, 2),
                'frames_arrived': self.last_frame_id[cam_id]
            }
        
        # 2. é€Ÿåº¦æ¯”ä¾‹
        if fps_values and min(fps_values) > 0:
            max_fps = max(fps_values)
            min_fps = min(fps_values)
            speed_ratio = max_fps / min_fps
            report['speed_ratio'] = round(speed_ratio, 2)
            report['speed_imbalance'] = "ä¸¥é‡" if speed_ratio > 1.5 else ("ä¸­ç­‰" if speed_ratio > 1.2 else "æ­£å¸¸")
        
        # 3. æ£€æµ‹é˜¶æ®µç»Ÿè®¡
        for cam_id in [1, 2, 3]:
            if self.detection_times[cam_id]:
                det_times = list(self.detection_times[cam_id])
                report['processing_stages'][f'detection_C{cam_id}'] = {
                    'avg_ms': round(mean(det_times), 2),
                    'max_ms': round(max(det_times), 2),
                    'min_ms': round(min(det_times), 2),
                    'count': len(det_times)
                }
        
        # 4. è·Ÿè¸ªé˜¶æ®µç»Ÿè®¡
        for cam_id in [1, 2, 3]:
            if self.tracking_times[cam_id]:
                track_times = list(self.tracking_times[cam_id])
                report['processing_stages'][f'tracking_C{cam_id}'] = {
                    'avg_ms': round(mean(track_times), 2),
                    'max_ms': round(max(track_times), 2),
                    'min_ms': round(min(track_times), 2),
                    'count': len(track_times)
                }
        
        # 5. èåˆé˜¶æ®µç»Ÿè®¡
        if self.fusion_times:
            fusion_times_list = list(self.fusion_times)
            report['processing_stages']['fusion_all'] = {
                'avg_ms': round(mean(fusion_times_list), 2),
                'max_ms': round(max(fusion_times_list), 2),
                'min_ms': round(min(fusion_times_list), 2),
                'count': len(fusion_times_list)
            }
        
        return report
    
    def print_speed_report(self):
        """æ‰“å°æ ¼å¼åŒ–çš„é€Ÿåº¦æŠ¥å‘Š"""
        stats = self.get_speed_statistics()
        
        current_time = time.time()
        elapsed = current_time - self.last_report_time
        
        if elapsed < self.report_interval:
            return ""
        
        self.last_report_time = current_time
        
        report_lines = []
        report_lines.append("\n" + "="*70)
        report_lines.append("ğŸš€ å®æ—¶é€Ÿåº¦ç›‘æ§æŠ¥å‘Š")
        report_lines.append("="*70)
        
        # æ‘„åƒå¤´é€Ÿåº¦
        report_lines.append("\nğŸ“¹ æ‘„åƒå¤´å¸§åˆ°è¾¾é€Ÿåº¦:")
        for cam_name, cam_data in stats['cameras'].items():
            growth_indicator = "ğŸ“ˆ" if cam_data['buffer_growth_rate'] > 2 else ("âš ï¸" if cam_data['buffer_growth_rate'] > 0.5 else "âœ…")
            report_lines.append(f"  {cam_name}: {cam_data['real_fps']:.2f} FPS | ç¼“å†²å¢é€Ÿ: {cam_data['buffer_growth_rate']:.2f}å¸§/s {growth_indicator}")
        
        # é€Ÿåº¦å¤±è¡¡è­¦å‘Š
        if 'speed_ratio' in stats:
            ratio = stats['speed_ratio']
            imbalance = stats['speed_imbalance']
            emoji = "ğŸ”´" if imbalance == "ä¸¥é‡" else ("ğŸŸ¡" if imbalance == "ä¸­ç­‰" else "ğŸŸ¢")
            report_lines.append(f"\nâš™ï¸  é€Ÿåº¦å¤±è¡¡åº¦: {ratio:.2f}x [{imbalance}] {emoji}")
        
        # æ£€æµ‹é˜¶æ®µè€—æ—¶
        detection_entries = {k: v for k, v in stats['processing_stages'].items() if 'detection' in k}
        if detection_entries:
            report_lines.append("\nğŸ” æ£€æµ‹é˜¶æ®µè€—æ—¶:")
            for stage_name, stage_data in detection_entries.items():
                cam_name = stage_name.replace('detection_', '')
                report_lines.append(f"  {cam_name}: {stage_data['avg_ms']:.2f}ms (min:{stage_data['min_ms']:.2f}, max:{stage_data['max_ms']:.2f})")
        
        # è·Ÿè¸ªé˜¶æ®µè€—æ—¶
        tracking_entries = {k: v for k, v in stats['processing_stages'].items() if 'tracking' in k}
        if tracking_entries:
            report_lines.append("\nğŸ‘ï¸  è·Ÿè¸ªé˜¶æ®µè€—æ—¶:")
            for stage_name, stage_data in tracking_entries.items():
                cam_name = stage_name.replace('tracking_', '')
                report_lines.append(f"  {cam_name}: {stage_data['avg_ms']:.2f}ms (min:{stage_data['min_ms']:.2f}, max:{stage_data['max_ms']:.2f})")
        
        # èåˆé˜¶æ®µè€—æ—¶
        if 'fusion_all' in stats['processing_stages']:
            fusion_data = stats['processing_stages']['fusion_all']
            report_lines.append("\nğŸ”— èåˆé˜¶æ®µè€—æ—¶:")
            report_lines.append(f"  å¹³å‡: {fusion_data['avg_ms']:.2f}ms (min:{fusion_data['min_ms']:.2f}, max:{fusion_data['max_ms']:.2f})")
        
        # æ•´ä½“è€—æ—¶è®¡ç®—
        total_detection = 0
        total_tracking = 0
        count = 0
        for cam_id in [1, 2, 3]:
            if self.detection_times[cam_id]:
                total_detection += sum(self.detection_times[cam_id])
                total_tracking += sum(self.tracking_times[cam_id])
                count += len(self.detection_times[cam_id])
        
        if count > 0:
            report_lines.append("\nğŸ“Š æ€»ä½“è€—æ—¶æ„æˆ:")
            report_lines.append(f"  æ£€æµ‹: {total_detection/count:.2f}ms/å¸§")
            report_lines.append(f"  è·Ÿè¸ª: {total_tracking/count:.2f}ms/å¸§")
            if self.fusion_times:
                report_lines.append(f"  èåˆ: {mean(self.fusion_times):.2f}ms/å¸§")
            report_lines.append(f"  åˆè®¡: {(total_detection + total_tracking)/count + (mean(self.fusion_times) if self.fusion_times else 0):.2f}ms/å¸§")
        
        # å»ºè®®
        report_lines.append("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if 'speed_ratio' in stats and stats['speed_ratio'] > 1.5:
            report_lines.append("  âš ï¸  æ‘„åƒå¤´å¤„ç†é€Ÿåº¦å·®å¼‚å¤§ï¼Œå»ºè®®æ£€æŸ¥SDKé…ç½®æˆ–æ¨¡å‹é€‰æ‹©")
        
        detection_max = max([v['max_ms'] for v in detection_entries.values()]) if detection_entries else 0
        if detection_max > 50:
            report_lines.append("  âš ï¸  æ£€æµ‹è€—æ—¶è¿‡é•¿ï¼Œè€ƒè™‘é™ä½æ¨¡å‹ç²¾åº¦æˆ–ç¼©å°è¾“å…¥åˆ†è¾¨ç‡")
        
        tracking_max = max([v['max_ms'] for v in tracking_entries.values()]) if tracking_entries else 0
        if tracking_max > 20:
            report_lines.append("  âš ï¸  è·Ÿè¸ªè€—æ—¶è¿‡é•¿ï¼Œè€ƒè™‘ä¼˜åŒ–ByteTrackerå‚æ•°")
        
        if self.fusion_times and mean(self.fusion_times) > 30:
            report_lines.append("  âš ï¸  èåˆè€—æ—¶è¿‡é•¿ï¼Œè€ƒè™‘ä¼˜åŒ–èåˆç®—æ³•")
        
        report_lines.append("="*70)
        
        return "\n".join(report_lines)

# ==================== å¸§æ±‡èšå™¨ (Frame Assembler) ====================
class FrameAssembler:
    """
    ç¬¬äºŒé˜¶æ®µï¼šå¸§æ±‡èšå™¨
    èŒè´£ï¼šä»ä¸‰ä¸ªæ£€æµ‹ç”Ÿäº§è€…çš„é˜Ÿåˆ—è¯»å–æ£€æµ‹ç»“æœï¼Œå½“æŸä¸€å¸§å·çš„ä¸‰è·¯æ•°æ®é½å…¨æ—¶ï¼Œ
    æ‰“åŒ…è¾“å‡ºåˆ°èåˆé˜Ÿåˆ—ä¾›ç¬¬ä¸‰é˜¶æ®µï¼ˆè·Ÿè¸ªä¸èåˆï¼‰æ¶ˆè´¹ã€‚
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    - å†…éƒ¨ç»´æŠ¤ frame_buffer = {frame_id: {camera_id: detection_result}}
    - ä»ä¸‰ä¸ªé˜Ÿåˆ—ä¸­è¯»å–æ£€æµ‹ç»“æœï¼ŒæŒ‰ frame_id å­˜å‚¨
    - æ£€æŸ¥å®Œæ•´æ€§ï¼šlen(frame_buffer[frame_id]) == 3ï¼Ÿ
    - è‹¥é½å…¨ï¼Œæ‰“åŒ…åˆ°èåˆé˜Ÿåˆ—ï¼›è‹¥è¶…æ—¶ï¼Œæ”¾å¼ƒè¯¥å¸§
    """
    
    def __init__(self, detection_queues: Dict[int, multiprocessing.Queue],
                 fusion_queue: multiprocessing.Queue, 
                 num_cameras: int = 3,
                 fps: int = 25,
                 timeout_frames: int = 10):
        """
        Args:
            detection_queues: {camera_id -> Queue}ï¼Œæ¥è‡ªä¸‰ä¸ªæ£€æµ‹ç”Ÿäº§è€…çš„é˜Ÿåˆ—
            fusion_queue: è¾“å‡ºèåˆé˜Ÿåˆ—ï¼Œå‘é€é½å…¨çš„ä¸‰è·¯æ•°æ®
            num_cameras: æ‘„åƒå¤´æ•°é‡
            fps: è§†é¢‘å¸§ç‡ï¼Œç”¨äºè®¡ç®—è¶…æ—¶
            timeout_frames: è¶…æ—¶æ—¶é—´ï¼ˆä»¥å¸§æ•°è®¡ï¼‰ï¼Œå¦‚æœç­‰å¾…è¶…è¿‡ timeout_framesï¼Œæ”¾å¼ƒ
        """
        self.detection_queues = detection_queues
        self.fusion_queue = fusion_queue
        self.num_cameras = num_cameras
        self.fps = fps
        self.timeout_frames = timeout_frames
        self.timeout_ms = (timeout_frames / fps) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # å¸§ç¼“å†²åŒºï¼šframe_id -> {camera_id: detection_result}
        self.frame_buffer = {}
        
        # è®°å½•æ¯ä¸ªframeçš„åˆ°è¾¾æ—¶é—´ï¼Œç”¨äºè¶…æ—¶æ£€æµ‹
        self.frame_timestamps = {}  # frame_id -> arrival_time
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'frames_assembled': 0,      # å®Œæ•´çš„ä¸‰è·¯å¸§ç»„
            'frames_timeout': 0,        # è¶…æ—¶ä¸¢å¼ƒçš„å¸§
            'detections_received': 0,   # æ¥æ”¶çš„æ£€æµ‹ç»“æœæ€»æ•°
            'max_buffer_size': 0        # ç¼“å†²åŒºæœ€å¤§å°ºå¯¸
        }
        
        # ç¼“å†²åŒºå¤§å°é™åˆ¶
        self.max_buffer_size = 300
        
        print(f"âœ… å¸§æ±‡èšå™¨åˆå§‹åŒ–å®Œæˆ - {num_cameras}æ‘„åƒå¤´, FPS:{fps}, è¶…æ—¶:{timeout_frames}å¸§({self.timeout_ms:.1f}ms)")
    
    def add_detection(self, camera_id: int, detection_result: dict):
        """
        æ¥æ”¶ä¸€æ¡æ£€æµ‹ç»“æœï¼ˆæ¥è‡ªæŸä¸ªç”Ÿäº§è€…ï¼‰
        
        Args:
            camera_id: æ‘„åƒå¤´ID (1/2/3)
            detection_result: åŒ…å« frame_id å’Œ detections çš„ç»“æœå­—å…¸
        """
        frame_id = detection_result.get('frame_id')
        if frame_id is None:
            print(f"âš ï¸  è­¦å‘Š: C{camera_id} æ£€æµ‹ç»“æœç¼ºå°‘ frame_idï¼Œå·²ä¸¢å¼ƒ")
            return
        
        # åˆå§‹åŒ–è¯¥å¸§çš„ç¼“å†²åŒºï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if frame_id not in self.frame_buffer:
            self.frame_buffer[frame_id] = {}
            self.frame_timestamps[frame_id] = time.time()
        
        # å­˜å‚¨æ£€æµ‹ç»“æœ
        self.frame_buffer[frame_id][camera_id] = detection_result
        self.stats['detections_received'] += 1
        
        # æ›´æ–°ç¼“å†²åŒºç»Ÿè®¡
        self.stats['max_buffer_size'] = max(self.stats['max_buffer_size'], len(self.frame_buffer))
    
    def try_assemble(self) -> Optional[dict]:
        """
        å°è¯•ç»„è£…ä¸€ä¸ªå®Œæ•´çš„ä¸‰è·¯å¸§ã€‚è¿”å›é½å…¨çš„å¸§ï¼Œæˆ– Noneã€‚
        
        Returns:
            å¦‚æœæŸä¸ªå¸§å·çš„ä¸‰è·¯æ•°æ®é½å…¨ï¼Œè¿”å›æ‰“åŒ…çš„å­—å…¸ï¼›å¦åˆ™è¿”å› None
            æ‰“åŒ…æ ¼å¼ï¼š{frame_id: int, cameras: {1: result, 2: result, 3: result}}
        """
        current_time = time.time()
        
        # éå†ç¼“å†²åŒºï¼Œå¯»æ‰¾é½å…¨æˆ–è¶…æ—¶çš„å¸§
        frame_ids_to_process = sorted(self.frame_buffer.keys())
        
        for frame_id in frame_ids_to_process:
            frame_data = self.frame_buffer[frame_id]
            arrival_time = self.frame_timestamps[frame_id]
            elapsed_ms = (current_time - arrival_time) * 1000
            
            # æ£€æŸ¥æ˜¯å¦é½å…¨
            if len(frame_data) == self.num_cameras:
                # å®Œæ•´çš„ä¸‰è·¯å¸§ï¼Œç›´æ¥è¿”å›
                assembled_frame = {
                    'frame_id': frame_id,
                    'cameras': frame_data,
                    'assembled_time': current_time
                }
                
                # ä»ç¼“å†²åŒºç§»é™¤
                del self.frame_buffer[frame_id]
                del self.frame_timestamps[frame_id]
                
                self.stats['frames_assembled'] += 1
                return assembled_frame
            
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if elapsed_ms > self.timeout_ms:
                # è¶…æ—¶äº†ï¼Œæ”¾å¼ƒè¿™ä¸€å¸§
                print(f"â±ï¸  Frame {frame_id} è¶…æ—¶ ({elapsed_ms:.1f}ms > {self.timeout_ms:.1f}ms), å·²æœ‰ {len(frame_data)}/3 æ‘„åƒå¤´æ•°æ®ï¼Œæ”¾å¼ƒ")
                
                # ä»ç¼“å†²åŒºç§»é™¤
                del self.frame_buffer[frame_id]
                del self.frame_timestamps[frame_id]
                
                self.stats['frames_timeout'] += 1
                # ä¸è¿”å›ï¼Œç»§ç»­æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå¯èƒ½é½å…¨çš„å¸§
                continue
        
        # æ²¡æœ‰é½å…¨æˆ–è¶…æ—¶çš„å¸§
        return None
    
    def run_loop(self, stop_event: threading.Event):
        """
        åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œçš„ä¸»å¾ªç¯ã€‚æŒç»­ç›‘å¬ä¸‰ä¸ªæ£€æµ‹é˜Ÿåˆ—ï¼Œå¹¶å°è¯•ç»„è£…å¸§ã€‚
        """
        print("ğŸš€ FrameAssembler çº¿ç¨‹å¯åŠ¨")
        
        while not stop_event.is_set():
            # ä»ä¸‰ä¸ªé˜Ÿåˆ—ä¸­è¯»å–æ‰€æœ‰å¯ç”¨çš„æ£€æµ‹ç»“æœ
            for camera_id in [1, 2, 3]:
                try:
                    detection_result = self.detection_queues[camera_id].get_nowait()
                    self.add_detection(camera_id, detection_result)
                except:
                    # é˜Ÿåˆ—ä¸ºç©ºæˆ–å…¶ä»–é”™è¯¯ï¼Œç»§ç»­
                    pass
            
            # å°è¯•ç»„è£…é½å…¨çš„å¸§
            assembled_frame = self.try_assemble()
            while assembled_frame:
                # å‘é€åˆ°èåˆé˜Ÿåˆ—
                try:
                    self.fusion_queue.put(assembled_frame, timeout=0.1)
                except:
                    # èåˆé˜Ÿåˆ—æ»¡ï¼Œç­‰å¾…ä¸€ä¸‹
                    time.sleep(0.001)
                    continue
                
                # å°è¯•ç»§ç»­ç»„è£…
                assembled_frame = self.try_assemble()
            
            # çŸ­æš‚ä¼‘çœ ï¼Œé¿å… CPU å ç”¨è¿‡é«˜
            time.sleep(0.001)
        
        print("ğŸ›‘ FrameAssembler çº¿ç¨‹å·²é€€å‡º")
    
    def get_statistics(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'frames_assembled': self.stats['frames_assembled'],
            'frames_timeout': self.stats['frames_timeout'],
            'detections_received': self.stats['detections_received'],
            'buffer_size': len(self.frame_buffer),
            'max_buffer_size': self.stats['max_buffer_size']
        }

# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    
    # 1. é…ç½®
    print("ğŸš€ ç¨‹åºå¼€å§‹å¯åŠ¨ - é‡æ„ç‰ˆèåˆç³»ç»Ÿ")
    
    # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–RTSP URLsï¼Œå¤±è´¥åˆ™ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶
    video_paths = {}
    if RTSP_MQTT_AVAILABLE:
        try:
            config_reader = ConfigReader()
            enabled_cameras = config_reader.get_enabled_cameras()
            
            if enabled_cameras and len(enabled_cameras) >= 3:
                for i, camera in enumerate(enabled_cameras[:3], 1):
                    video_paths[i] = camera['rtsp_url']
                    print(f"ğŸ“· æ‘„åƒå¤´ {i}: {camera['name']} - {camera['rtsp_url']}")
                print("âœ… ä½¿ç”¨RTSPæµä½œä¸ºè¾“å…¥")
            else:
                print("âš ï¸  é…ç½®æ–‡ä»¶ä¸­æ‘„åƒå¤´æ•°é‡ä¸è¶³ï¼Œå›é€€åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶")
                raise Exception("é…ç½®ä¸è¶³")
        except Exception as e:
            print(f"âš ï¸  è¯»å–RTSPé…ç½®å¤±è´¥: {e}, ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶")
            RTSP_MQTT_AVAILABLE = False
    
    # å›é€€åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶
    if not RTSP_MQTT_AVAILABLE or not video_paths:
        print("âŒ RTSPæ‹‰æµå¤±è´¥ï¼Œéœ€è¦é…ç½®æœ¬åœ°è§†é¢‘è·¯å¾„")
        # è¿™é‡Œéœ€è¦ç”¨æˆ·æ‰‹åŠ¨é…ç½®è§†é¢‘è·¯å¾„
        video_paths = {
            1: "/path/to/video1.mp4",
            2: "/path/to/video2.mp4",
            3: "/path/to/video3.mp4"
        }

    # æ£€æµ‹åŒºåŸŸï¼ˆåƒç´ åæ ‡ï¼‰
    detect_areas = {
        1: [np.array([[0, 1080], [1920, 1080], [1920, 592], [1108, 139], [726, 125], [0, 797]], dtype=np.int32),
            np.array([[64, 647], [355, 474], [621, 224], [494, 228]], dtype=np.int32)],
        2: [np.array([[0, 792], [716, 181], [1238, 175], [1920, 628], [1920, 1080]], dtype=np.int32)],
        3: [np.array([[0, 1080], [1185, 1080], [988, 257], [0, 276]], dtype=np.int32)]
    }
        
    signal.signal(signal.SIGINT, cancel_process)
    
    # 2. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
    fusion_system = CrossCameraFusion()
    queues = {i: multiprocessing.Queue(maxsize=10) for i in [1, 2, 3]}
    perf_monitor = PerformanceMonitor()
    
    # æ–°å¢ï¼šåˆå§‹åŒ–é€Ÿåº¦ç›‘æ§
    speed_monitor = SpeedMonitor(window_size=300)
    
    # åˆå§‹åŒ–MQTTå‘å¸ƒå™¨
    mqtt_publisher = None
    if RTSP_MQTT_AVAILABLE:
        try:
            mqtt_publisher = MqttPublisher("config/mqtt_config.ini")
            mqtt_publisher.connect()
            print("âœ… MQTTå‘å¸ƒå™¨å·²è¿æ¥")
        except Exception as e:
            print(f"âš ï¸  MQTTè¿æ¥å¤±è´¥: {e}, å°†ä½¿ç”¨JSONæ–‡ä»¶ä¿å­˜")
            mqtt_publisher = None
    
    # åˆå§‹åŒ–ByteTracker
    class TrackerArgs:
        def __init__(self):
            self.track_thresh = 0.3
            self.track_buffer = 15
            self.match_thresh = 0.8
            self.mot20 = False  # æ·»åŠ mot20å±æ€§ï¼Œç”¨äºåŒºåˆ†MOT20æ•°æ®é›†
    
    tracker_args = TrackerArgs()
    trackers = {i: BYTETracker(tracker_args, frame_rate=Config.FPS) for i in [1, 2, 3]}
    print("âœ… ByteTrackeråˆå§‹åŒ–å®Œæˆ") 

    # 3. åˆ›å»ºå¹¶å¯åŠ¨ SDK æ¨ç†è¿›ç¨‹
    processes = []
    print("ğŸš€ å¯åŠ¨SDKç‰ˆå¤šæ‘„åƒå¤´èåˆç³»ç»Ÿ")
    for camera_id in [1, 2, 3]:
        process = multiprocessing.Process(
            target=create_sdk_worker_process,
            args=(camera_id, video_paths[camera_id], queues[camera_id]),
            daemon=True
        )
        processes.append(process)
        process.start()
        print(f"ğŸ”„ å¯åŠ¨Camera{camera_id} SDKæ¨ç†è¿›ç¨‹...")
        
    print("âœ… æ‰€æœ‰SDKæ¨ç†è¿›ç¨‹å·²å¯åŠ¨")

    # 3.5. é¢„çƒ­é˜¶æ®µ
    print("\nâ±ï¸  è¿›å…¥é¢„çƒ­é˜¶æ®µï¼Œç­‰å¾…æ‰€æœ‰æ‘„åƒå¤´æ¨é€ç¬¬ä¸€å¸§æ•°æ®...")
    PREHEAT_TIMEOUT = 30
    start_time = time.time()
    ready_cameras = {i: False for i in [1, 2, 3]}
    
    while time.time() - start_time < PREHEAT_TIMEOUT:
        all_ready = True
        for cam_id in [1, 2, 3]:
            if not ready_cameras[cam_id] and not queues[cam_id].empty():
                ready_cameras[cam_id] = True
                print(f"âœ… æ‘„åƒå¤´ C{cam_id} å·²å°±ç»ªï¼")
            elif not ready_cameras[cam_id]:
                all_ready = False
        
        if all(ready_cameras.values()):
            print("ğŸ‰ æ‰€æœ‰æ‘„åƒå¤´å‡å·²å°±ç»ªï¼Œé¢„çƒ­å®Œæˆï¼")
            break
        
        time.sleep(0.5)
    else:
        print("âŒ é¢„çƒ­è¶…æ—¶ï¼")

    # 4. ä¸»å¾ªç¯ï¼šä¸‰é˜¶æ®µç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¶æ„
    max_frames = 500
    
    # åˆ›å»ºèåˆé˜Ÿåˆ—ï¼ˆç”¨äºé˜¶æ®µäºŒå’Œé˜¶æ®µä¸‰ä¹‹é—´çš„é€šä¿¡ï¼‰
    fusion_queue = multiprocessing.Queue(maxsize=20)
    
    # åˆ›å»ºå¸§æ±‡èšå™¨ï¼ˆé˜¶æ®µäºŒï¼šFrame Assemblerï¼‰
    frame_assembler = FrameAssembler(
        detection_queues=queues,
        fusion_queue=fusion_queue,
        num_cameras=3,
        fps=Config.FPS,
        timeout_frames=10
    )
    
    # å¯åŠ¨ FrameAssembler çº¿ç¨‹
    assembler_stop_event = threading.Event()
    assembler_thread = threading.Thread(
        target=frame_assembler.run_loop,
        args=(assembler_stop_event,),
        daemon=True,
        name="FrameAssembler"
    )
    assembler_thread.start()
    print("âœ… FrameAssembler çº¿ç¨‹å·²å¯åŠ¨")
    
    # æ–°å¢ï¼šè¯¦ç»†è®¡æ—¶ç»Ÿè®¡
    detailed_timing_stats = {
        'frame_sync': [],      # ä»èåˆé˜Ÿåˆ—è·å–å¸§çš„æ—¶é—´
        'detection': [],       # æ£€æµ‹è€—æ—¶
        'tracking': [],        # è·Ÿè¸ªè€—æ—¶
        'fusion': [],          # èåˆè€—æ—¶
        'json_save': [],       # JSONä¿å­˜è€—æ—¶
        'total': []            # æ€»è€—æ—¶
    }
    
    print("\n--- èåˆä¸»å¾ªç¯å¯åŠ¨ (ä¸‰é˜¶æ®µæ¶æ„) ---")

    try:
        current_frame = 0
        last_assembled_frame_id = -1
        
        while not cancel_flag.value and current_frame < max_frames:
            # ============ ä¸»å¾ªç¯æ€»è€—æ—¶è®¡æ—¶å¼€å§‹ ============
            loop_start_time = time.time()
            
            # A. ä»èåˆé˜Ÿåˆ—è·å–å®Œæ•´çš„ä¸‰è·¯æ•°æ®
            # ============ å¸§åŒæ­¥è®¡æ—¶ ============
            frame_sync_start = time.time()
            
            try:
                assembled_frame = fusion_queue.get(timeout=0.5)
            except:
                # èåˆé˜Ÿåˆ—è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…
                time.sleep(0.001)
                continue
            
            frame_sync_time = (time.time() - frame_sync_start) * 1000
            detailed_timing_stats['frame_sync'].append(frame_sync_time)
            
            # è§£åŒ…ç»„è£…å¥½çš„å¸§
            frame_id = assembled_frame['frame_id']
            cameras_data = assembled_frame['cameras']  # {1: result, 2: result, 3: result}
            
            current_frame = frame_id
            perf_monitor.add_counter('frames_synchronized')
            perf_monitor.add_counter('frames_processed')
            
            # B. ä¸ºæ¯ä¸ªæ‘„åƒå¤´è¿›è¡Œæ£€æµ‹ã€è·Ÿè¸ªã€èåˆå¤„ç†
            all_global_targets_this_frame = []
            all_local_targets_this_frame = []
            
            # ============ æ£€æµ‹å’Œè·Ÿè¸ªè®¡æ—¶ç»Ÿè®¡ ============
            detection_times = []
            tracking_times = []
            
            perf_monitor.start_timer('frame_processing')
            
            for camera_id in [1, 2, 3]:
                result = cameras_data[camera_id]
                perf_monitor.start_timer(f'camera_{camera_id}_processing')
                
                # è®°å½•å¸§åˆ°è¾¾
                speed_monitor.record_frame_arrival(camera_id, frame_id)
                
                # 1. ç±»åˆ«è¿‡æ»¤å’ŒåŒºåŸŸè¿‡æ»¤
                raw_detections = [d for d in result['detections'] 
                                if d['class'] in Config.VEHICLE_CLASSES and d['class'] not in Config.EXCLUDE_CLASSES]
                perf_monitor.add_counter('detections_processed', len(raw_detections))

                filtered_detections = filter_by_detect_areas(raw_detections, detect_areas[camera_id])

                # 2. NMS
                perf_monitor.start_timer(f'detection_{camera_id}')
                det_for_nms = [{'box': d['box'], 'confidence': d['confidence'], 'class': d['class']} for d in filtered_detections]
                nms_detections = DetectionUtils.non_max_suppression(det_for_nms)
                detection_time = perf_monitor.end_timer(f'detection_{camera_id}')
                
                # è®°å½•æ£€æµ‹è€—æ—¶
                speed_monitor.record_detection_time(camera_id, detection_time)
                detection_times.append(detection_time)
                
                # 3. è·Ÿè¸ªå™¨è¾“å…¥
                tracker_input_tensor = batch_prepare_tracker_input(nms_detections)
                
                # 4. å±€éƒ¨è·Ÿè¸ª
                perf_monitor.start_timer(f'tracker_update_{camera_id}')
                
                # ByteTrackerçš„updateæ–¹æ³•éœ€è¦3ä¸ªå‚æ•°ï¼šoutput_results, img_info, img_size
                img_info = [Config.IMAGE_HEIGHT, Config.IMAGE_WIDTH]  # [height, width]
                img_size = (Config.IMAGE_HEIGHT, Config.IMAGE_WIDTH)  # (height, width)
                tracked_objects = trackers[camera_id].update(tracker_input_tensor, img_info, img_size)

                tracker_time = perf_monitor.end_timer(f'tracker_update_{camera_id}')
                perf_monitor.add_counter('tracker_updates')
                
                # è®°å½•è·Ÿè¸ªè€—æ—¶
                speed_monitor.record_tracking_time(camera_id, tracker_time)
                tracking_times.append(tracker_time)

                # 5. è·Ÿè¸ªç»“æœè½¬æ¢
                tracked_detections = batch_convert_track_results(tracked_objects, result, camera_id, current_frame, nms_detections)
                
                # 6. åˆ†ç±»å¤„ç†
                global_targets, local_targets = fusion_system.process_detections(tracked_detections, camera_id, perf_monitor)
                
                all_global_targets_this_frame.extend(global_targets)
                all_local_targets_this_frame.extend(local_targets)
                
                camera_processing_time = perf_monitor.end_timer(f'camera_{camera_id}_processing')
                perf_monitor.record_fusion_stats(f'camera_{camera_id}_processing', camera_processing_time, {
                    'raw_detections': len(raw_detections),
                    'tracked_detections': len(tracked_detections),
                    'global_targets': len(global_targets),
                    'local_targets': len(local_targets)
                })
            
            frame_processing_time = perf_monitor.end_timer('frame_processing')
            detailed_timing_stats['detection'].append(sum(detection_times))
            detailed_timing_stats['tracking'].append(sum(tracking_times))

            # C. åŒ¹é…èåˆï¼ˆè·¨æ‘„åƒå¤´ï¼‰
            perf_monitor.start_timer('matching_processing')
            active_global_targets = list(fusion_system.global_targets.values())
            fusion_system._perform_matching(all_local_targets_this_frame, active_global_targets)
            
            # æ›´æ–°å…¨å±€çŠ¶æ€
            fusion_system.update_global_state(all_global_targets_this_frame, all_local_targets_this_frame)
            matching_time = perf_monitor.end_timer('matching_processing')
            
            # è®°å½•èåˆè€—æ—¶
            speed_monitor.record_fusion_time(matching_time)
            detailed_timing_stats['fusion'].append(matching_time)
            
            # æ”¶é›†æ‰€æœ‰ç›®æ ‡ç”¨äºJSONè¾“å‡º
            all_frame_detections = all_global_targets_this_frame + all_local_targets_this_frame

            # D. ç”ŸæˆJSONæ•°æ®å¹¶å°è¯•å‘é€MQTT
            # ============ JSONä¿å­˜è®¡æ—¶ ============
            json_save_start = time.time()
            
            perf_monitor.start_timer('json_mqtt_processing')
            json_data = fusion_system.generate_json_data_new(all_global_targets_this_frame, all_local_targets_this_frame)
            
            mqtt_sent = False
            if mqtt_publisher:
                try:
                    participants = json_data.get('participant', [])
                    mqtt_sent = mqtt_publisher.publish_rsm(participants)
                    if mqtt_sent:
                        perf_monitor.add_counter('mqtt_sends')
                    else:
                        perf_monitor.add_counter('mqtt_failures')
                except Exception as e:
                    print(f"âŒ MQTTå‘é€å¼‚å¸¸: {e}")
                    perf_monitor.add_counter('mqtt_failures')
                    mqtt_sent = False
            
            if not mqtt_sent:
                fusion_system.json_output_data.append(json_data)
            
            # å®æ—¶å†™å…¥JSONæ–‡ä»¶
            try:
                fusion_system.save_json_data_realtime("output_fusion_refactored.json", current_frame)
            except Exception as e:
                print(f"âŒ å®æ—¶JSONä¿å­˜å¤±è´¥: {e}")
            
            json_mqtt_time = perf_monitor.end_timer('json_mqtt_processing')
            detailed_timing_stats['json_save'].append(json_mqtt_time)
            
            # æ‰“å°å¤„ç†ä¿¡æ¯
            print(f"âœ… èåˆå¸§ {current_frame} | ç›®æ ‡æ•°: {len(all_frame_detections)} | MQTT: {'æˆåŠŸ' if mqtt_sent else 'å¤±è´¥/æœªé…ç½®'}")
            
            fusion_system.next_frame()
            
            # å®šæœŸè¾“å‡ºæ€§èƒ½æŠ¥å‘Š
            perf_report = perf_monitor.get_performance_report()
            if perf_report:
                print(perf_report)
            
            # å®šæœŸè¾“å‡ºé€Ÿåº¦ç›‘æ§æŠ¥å‘Š
            speed_report = speed_monitor.print_speed_report()
            if speed_report:
                print(speed_report)

            # E. å®šæœŸæŠ¥å‘Šæ±‡èšå™¨ç»Ÿè®¡
            if current_frame > 0 and current_frame % 300 == 0:
                assembler_stats = frame_assembler.get_statistics()
                print(f"\nğŸ“Š ----- FrameAssembler ç»Ÿè®¡ (æˆªè‡³Frame {current_frame}) -----")
                print(f"  å·²ç»„è£…å¸§: {assembler_stats['frames_assembled']}")
                print(f"  è¶…æ—¶å¸§: {assembler_stats['frames_timeout']}")
                print(f"  æ¥æ”¶æ£€æµ‹æ•°: {assembler_stats['detections_received']}")
                print(f"  å½“å‰ç¼“å†²åŒºå¤§å°: {assembler_stats['buffer_size']}")
                print(f"  æœ€å¤§ç¼“å†²åŒºå¤§å°: {assembler_stats['max_buffer_size']}")
                print("-" * 50)
        
        print("\nğŸ¯ æ‰€æœ‰å¤„ç†å®Œæˆ (æˆ–è¾¾åˆ°æœ€å¤§å¸§æ•°)")
        
        # 5. ä¿å­˜èåˆç»“æœ
        fusion_system.save_json_data("output_fusion_refactored.json")
        
        # 6. è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š:")
        
        assembler_stats = frame_assembler.get_statistics()
        synchronized_frames_count = assembler_stats['frames_assembled']

        print(f"ğŸ“ˆ å¤„ç†æ¦‚å†µ:")
        print(f"  æˆåŠŸç»„è£…å¸§ç»„: {synchronized_frames_count} ç»„")
        print(f"  è¶…æ—¶ä¸¢å¼ƒå¸§: {assembler_stats['frames_timeout']} ä¸ª")
        print(f"  æ¥æ”¶æ£€æµ‹æ€»æ•°: {assembler_stats['detections_received']}")
        print(f"  Globalè½¦è¾†æ± å¤§å°: {len(fusion_system.global_targets)}")
        print(f"  Localè½¦è¾†æ± å¤§å°: {len(fusion_system.local_track_buffer)}")
        print(f"  ç¡®è®¤ç›®æ ‡æ•°: {len(fusion_system.confirmed_targets)}")

        # è¾“å‡ºè¯¦ç»†è®¡æ—¶ç»Ÿè®¡
        print(f"\nğŸ“Š è¯¦ç»†è®¡æ—¶ç»Ÿè®¡:")
        for stage, times in detailed_timing_stats.items():
            if times:
                print(f"  {stage}: å¹³å‡{sum(times)/len(times):.2f}ms, æœ€å¤§{max(times):.2f}ms, æœ€å°{min(times):.2f}ms, æ¬¡æ•°{len(times)}")

        # è¾“å‡ºè¯¦ç»†å¤„ç†æ—¶é—´åˆ†è§£æŠ¥å‘Š
        print("\n" + "="*70)
        print("ğŸ“Š è¯¦ç»†å¤„ç†æ—¶é—´åˆ†è§£æŠ¥å‘Š")
        print("="*70)
        
        if any(detailed_timing_stats.values()):
            print("\nâ±ï¸  å„é˜¶æ®µå¹³å‡è€—æ—¶:")
            total_accounted = 0
            for stage_name, times in detailed_timing_stats.items():
                if times:
                    avg_time = sum(times) / len(times)
                    max_time = max(times)
                    min_time = min(times)
                    total_accounted += avg_time
                    print(f"  {stage_name:15} | å¹³å‡: {avg_time:7.2f}ms | æœ€å¤§: {max_time:7.2f}ms | æœ€å°: {min_time:7.2f}ms | æ ·æœ¬: {len(times):3d}")
            
            print(f"\n  {'å„é˜¶æ®µåˆè®¡':15} | æ€»è€—æ—¶: {total_accounted:.2f}ms")
            print(f"  {'å¤„ç†é€Ÿåº¦':15} | {1000.0 / total_accounted:.2f} FPS")
            
            print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            if synchronized_frames_count < 100:
                print(f"  âš ï¸  åŒæ­¥å¸§æ•°è¾ƒå°‘ ({synchronized_frames_count})ï¼Œå¯èƒ½æ‘„åƒå¤´é—´åŒæ­¥é—®é¢˜")
            if assembler_stats['frames_timeout'] > synchronized_frames_count * 0.1:
                print(f"  âš ï¸  è¶…æ—¶å¸§æ¯”ä¾‹è¿‡é«˜ ({assembler_stats['frames_timeout']}/{synchronized_frames_count})")
        
        print("="*70)

    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # 7. æ¸…ç†èµ„æº
        print("\nğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")
        
        # åœæ­¢ FrameAssembler çº¿ç¨‹
        assembler_stop_event.set()
        assembler_thread.join(timeout=5.0)
        
        # åœæ­¢ SDK æ¨ç†è¿›ç¨‹
        cancel_flag.value = True
        for process in processes:
            if process.is_alive():
                process.terminate()
                process.join(timeout=2.0)
        
        # å…³é—­å¼‚æ­¥JSONä¿å­˜å™¨
        if 'fusion_system' in locals():
            fusion_system.json_saver.shutdown()

        if mqtt_publisher:
            try:
                mqtt_publisher.disconnect()
                print("âœ… MQTTè¿æ¥å·²æ–­å¼€")
            except:
                pass
                
        print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")