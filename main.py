#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SDKç‰ˆå¤šæ‘„åƒå¤´èåˆç³»ç»Ÿ - é‡æ„ç‰ˆ
èŒè´£åˆ†ç¦»: 
1. å­è¿›ç¨‹ (yolov5_SDK): ä»…è´Ÿè´£è§†é¢‘è¯»å–ã€SDKæ¨ç†ã€ç»“æœå…¥é˜Ÿåˆ— (ä½¿ç”¨ SDKinfer_ffmpeg.py)
2. ä¸»è¿›ç¨‹ (main): è´Ÿè´£è·Ÿè¸ª(BYTETracker)ã€åŒºåŸŸè¿‡æ»¤ã€è·¨æ‘„åƒå¤´èåˆã€å¸§åŒæ­¥ 
"""

import os
import sys
import time
import multiprocessing
import copy
import json
import signal
import logging
from collections import defaultdict, deque
from statistics import mean, median
sys.path.append('/usr/local/lynxi/sdk/sdk-samples/python')

# é…ç½®logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('fusion_system.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

import numpy as np
import cv2
from ctypes import *
import ctypes
from dataclasses import dataclass
from typing import List, Tuple, Optional, Set, Dict

# å¯¼å…¥SDKç›¸å…³æ¨¡å—
import pycommon.common as common
import pylynchipsdk as sdk
from pycommon.infer_process import *
from pycommon.callback_data_struct import *
from pycommon.dump_json import *
from ByteTrack.optimized_byte_tracker import OptimizedBYTETracker as BYTETracker

# å¯¼å…¥RTSPå’ŒMQTTç›¸å…³æ¨¡å— (æ–°å¢)
try:
    from rtsp_reader import RTSPStreamReader
    from mqtt_publisher import MqttPublisher  
    from config_reader import ConfigReader
    RTSP_MQTT_AVAILABLE = True
except ImportError as e:
    logger.warning(f"æ— æ³•å¯¼å…¥RTSP/MQTTæ¨¡å—: {e}, å°†ä½¿ç”¨æœ¬åœ°è§†é¢‘æ¨¡å¼")
    RTSP_MQTT_AVAILABLE = False

from Timestamp_sync import FFmpegTimeStampProvider, FFmpegTimestampFrameSynchronizer
from Basic import Config, DetectionUtils, GeometryUtils, PerformanceMonitor
from TargetTrack import TargetBuffer
from Fusion import CrossCameraFusion
from FrameSynchronizer import FrameLossPrevention
from RadarVisionFusion import RadarVisionFusionProcessor, RadarDataLoader, OutputObject

# åˆ›å»ºå…±äº«å¸ƒå°”å€¼ç”¨äºåœæ­¢è¿è¡Œçº¿ç¨‹
cancel_flag = multiprocessing.Value('b', False)
# --- è¾…åŠ©å‡½æ•° (ä»SDKinfer.pyç§»è¿‡æ¥) ---

def cancel_process(signum, frame):
    """å–æ¶ˆå¤„ç†ä¿¡å·"""
    global cancel_flag
    cancel_flag.value = True
    logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")

def filter_by_detect_areas(detections: List[dict], areas: List[np.ndarray]) -> List[dict]:
    """æ ¹æ®æ£€æµ‹åŒºåŸŸè¿‡æ»¤æ£€æµ‹ç»“æœ"""
    filtered_detections = []
    for detection in detections:
        # ä½¿ç”¨è¾¹ç•Œæ¡†çš„åº•éƒ¨ä¸­å¿ƒç‚¹ä½œä¸ºåˆ¤æ–­ç‚¹
        x1, y1, x2, y2 = detection['box']
        center_x, center_y = int((x1 + x2) / 2), int(y2) 
        in_detect_area = any(cv2.pointPolygonTest(area, (center_x, center_y), False) >= 0 
                           for area in areas)
        if in_detect_area:
            filtered_detections.append(detection)
    return filtered_detections

def batch_prepare_tracker_input(nms_detections: List[dict]) -> Tuple[np.ndarray, Dict[str, str]]:
    """
    æ‰¹é‡å‡†å¤‡è·Ÿè¸ªå™¨è¾“å…¥ï¼Œä¼˜åŒ–æ€§èƒ½ - ä½¿ç”¨numpyé¿å…torchä¾èµ–
    åŒæ—¶è¿”å›æ£€æµ‹æ¡†åˆ°ç±»åˆ«çš„æ˜ å°„å­—å…¸
    """
    if not nms_detections:
        return np.empty((0, 5), dtype=np.float32), {}
    
    # æ‰¹é‡æå–æ•°æ®ï¼Œé¿å…å¾ªç¯
    # BYTETracker æœŸæœ›çš„æ ¼å¼: [x1, y1, x2, y2, score]
    boxes_scores = np.array([[d['box'][0], d['box'][1], d['box'][2], d['box'][3], d['confidence']] 
                              for d in nms_detections], dtype=np.float32)
    
    # åˆ›å»ºæ˜ å°„å­—å…¸ï¼škeyä¸ºboxçš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œvalueä¸ºç±»åˆ«åç§°
    # è¿™æ ·å¯ä»¥é€šè¿‡boxåæ ‡å¿«é€ŸæŸ¥æ‰¾å¯¹åº”çš„ç±»åˆ«
    box_to_class = {}
    for i, det in enumerate(nms_detections):
        box_key = f"{det['box'][0]:.1f}_{det['box'][1]:.1f}_{det['box'][2]:.1f}_{det['box'][3]:.1f}"
        box_to_class[box_key] = det['class']
    
    # è·Ÿè¸ªå™¨åªéœ€è¦å‰5åˆ—ï¼š[x1, y1, x2, y2, score]
    tracker_input_array = boxes_scores.astype(np.float32)
    return tracker_input_array, box_to_class

def batch_convert_track_results(tracked_objects: List, result: dict, camera_id: int, current_frame: int, 
                               original_detections: List[dict] = None, box_to_class: Dict[str, str] = None) -> List[dict]:
    """
    æ‰¹é‡è½¬æ¢è·Ÿè¸ªç»“æœï¼Œä¼˜åŒ–æ€§èƒ½å¹¶ä¿ç•™åŸå§‹ç±»åˆ«ä¿¡æ¯
    box_to_class: æ£€æµ‹æ¡†åˆ°ç±»åˆ«çš„æ˜ å°„å­—å…¸
    """
    tracked_detections = []
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    if current_frame % 100 == 0 and len(tracked_objects) > 0:
        print(f"ğŸ” C{camera_id} Frame {current_frame}: {len(tracked_objects)} tracked objects, box_to_class={len(box_to_class) if box_to_class else 0}")
    
    for track in tracked_objects:
        # é«˜æ•ˆè½¬æ¢tlwhåˆ°tlbr
        tlwh = track.tlwh
        tlbr = [tlwh[0], tlwh[1], tlwh[0] + tlwh[2], tlwh[1] + tlwh[3]]
        
        # å°è¯•ä»box_to_classæ˜ å°„ä¸­è·å–ç±»åˆ«ä¿¡æ¯ï¼ˆæœ€å‡†ç¡®çš„æ–¹æ³•ï¼‰
        class_name = 'car'  # é»˜è®¤å€¼
        
        if box_to_class:
            # å°è¯•é€šè¿‡IoUåŒ¹é…æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ£€æµ‹
            best_iou = 0
            best_class = None
            best_box_key = None
            
            for box_key, class_str in box_to_class.items():
                # è§£æbox_key
                coords = box_key.split('_')
                if len(coords) == 4:
                    try:
                        orig_box = [float(c) for c in coords]
                        iou = GeometryUtils.calculate_iou(tlbr, orig_box)
                        if iou > best_iou and iou > 0.1:  # é™ä½IoUé˜ˆå€¼
                            best_iou = iou
                            best_class = class_str
                            best_box_key = box_key
                    except:
                        continue
            
            if best_class:
                class_name = best_class
                if box_to_class and best_box_key:
                    # åˆ é™¤å·²ä½¿ç”¨çš„æ˜ å°„ï¼Œé¿å…é‡å¤ä½¿ç”¨
                    del box_to_class[best_box_key]
            else:
                # å¦‚æœIoUåŒ¹é…å¤±è´¥ï¼Œå°è¯•é€šè¿‡è·ç¦»åŒ¹é…
                min_distance = float('inf')
                best_class_dist = None
                
                for box_key, class_str in box_to_class.items():
                    coords = box_key.split('_')
                    if len(coords) == 4:
                        try:
                            orig_box = [float(c) for c in coords]
                            orig_center = [(orig_box[0] + orig_box[2]) / 2, (orig_box[1] + orig_box[3]) / 2]
                            track_center = [(tlbr[0] + tlbr[2]) / 2, (tlbr[1] + tlbr[3]) / 2]
                            distance = ((orig_center[0] - track_center[0]) ** 2 + 
                                       (orig_center[1] - track_center[1]) ** 2) ** 0.5
                            
                            if distance < min_distance and distance < 100:  # 100åƒç´ å†…çš„æœ€è¿‘åŒ¹é…
                                min_distance = distance
                                best_class_dist = class_str
                        except:
                            continue
                
                if best_class_dist:
                    class_name = best_class_dist
        
        elif original_detections:
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹æ£€æµ‹åˆ—è¡¨
            best_iou = 0
            best_match = None
            for orig_det in original_detections:
                iou = GeometryUtils.calculate_iou(tlbr, orig_det['box'])
                if iou > best_iou and iou > 0.1:
                    best_iou = iou
                    best_match = orig_det
            
            if best_match:
                class_name = best_match['class']
            else:
                # è·ç¦»åŒ¹é…
                min_distance = float('inf')
                for orig_det in original_detections:
                    orig_center = [(orig_det['box'][0] + orig_det['box'][2]) / 2, 
                                  (orig_det['box'][1] + orig_det['box'][3]) / 2]
                    track_center = [(tlbr[0] + tlbr[2]) / 2, (tlbr[1] + tlbr[3]) / 2]
                    distance = ((orig_center[0] - track_center[0]) ** 2 + 
                               (orig_center[1] - track_center[1]) ** 2) ** 0.5
                    
                    if distance < min_distance and distance < 100:
                        min_distance = distance
                        class_name = orig_det['class']
        
        detection = {
            'box': tlbr,
            'confidence': track.score,
            'class': class_name,  # ä¿ç•™åŸå§‹ç±»åˆ«ä¿¡æ¯
            'track_id': track.track_id,
            'local_id': track.track_id,
            'center_point': [(tlbr[0] + tlbr[2]) / 2, (tlbr[1] + tlbr[3]) / 2],
            'timestamp': result.get('timestamp', time.time()),
            'frame_number': result.get('frame_number', current_frame),
            'camera_id': camera_id,
            'sync_id': result.get('sync_id', f"C{camera_id}_F{current_frame}")
        }
        tracked_detections.append(detection)
    
    return tracked_detections

# ğŸ”§ ä¿®æ”¹ï¼šç§»é™¤äº† timestamp_provider å‚æ•°
def create_sdk_worker_process(camera_id: int, video_path: str, result_queue: multiprocessing.Queue):
    """åˆ›å»ºå¹¶è¿è¡Œä¸€ä¸ªç‹¬ç«‹çš„ SDK æ¨ç†å­è¿›ç¨‹ (ç”Ÿäº§è€…)"""
    
    # ç¡®ä¿å­è¿›ç¨‹èƒ½æ‰¾åˆ° SDKinfer_ffmpeg æ¨¡å—
    from SDKinfer_ffmpeg import yolov5_SDK, infer_process_attr
    
    try:
        print(f"ğŸ”§ Camera{camera_id} å­è¿›ç¨‹å¯åŠ¨ï¼Œå‡†å¤‡åˆå§‹åŒ–SDK...")
        print(f"   è§†é¢‘æº: {video_path[:80] if len(video_path) > 80 else video_path}")
        
        attr = infer_process_attr()
        attr.url = video_path
        attr.device_id = 0
        attr.chan_id = camera_id - 1
        attr.plugin_path = "/usr/local/lynxi/sdk/sdk-samples/plugin/obj/libYolov5Plugin.so"
        attr.model_path = "/root/yolov5-7.0_lyngor1.17.0/best_yolov5s_onnx/Net_0/"
        
        print(f"ğŸ”§ Camera{camera_id} åˆå§‹åŒ–yolov5_SDK (V13_PyAV)...")
        print(f"   âš ï¸  å¦‚æœå‡ºç° 'av.open' é”™è¯¯ï¼Œè¯´æ˜æ— æ³•æ‰“å¼€è§†é¢‘æº")
        print(f"   è¯·æ£€æŸ¥: 1) RTSP URLæ˜¯å¦æ­£ç¡® 2) ç½‘ç»œè¿æ¥ 3) è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        
        # ğŸ”§ æ”¹è¿›ï¼šç§»é™¤ä¼ é€’ timestamp_provider
        # æ³¨æ„ï¼šSDKåˆå§‹åŒ–å¯èƒ½ä¼šåœ¨è¿™é‡Œå¤±è´¥ï¼Œå¦‚æœRTSPè¿æ¥ä¸å¯ç”¨
        worker = yolov5_SDK(attr, result_queue) 
        print(f"âœ… Camera{camera_id} yolov5_SDKåˆå§‹åŒ–æˆåŠŸ")
        worker.run(cancel_flag) # è¿è¡Œåœ¨å­è¿›ç¨‹ä¸­
        print(f"âœ… Camera{camera_id} å­è¿›ç¨‹æ­£å¸¸é€€å‡º")
        
    except Exception as e:
        error_msg = str(e)
        print(f"\nâŒ Camera{camera_id} SDKè¿›ç¨‹å¯åŠ¨æˆ–è¿è¡Œå¤±è´¥")
        print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
        import traceback
        traceback.print_exc()
        # ç¡®ä¿è¿›ç¨‹èƒ½é€€å‡º
        os._exit(1)
        
# --- ä¸»ç¨‹åº (æ¶ˆè´¹è€…ï¼Œèåˆä¸åŒæ­¥) ---

if __name__ == "__main__":
    
    # 1. é…ç½®
    logger.info("="*60)
    logger.info("ç¨‹åºå¼€å§‹å¯åŠ¨...")
    logger.info("="*60)
    
    # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–RTSP URLsï¼Œå¤±è´¥åˆ™ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶
    video_paths = {}
    if RTSP_MQTT_AVAILABLE:
        try:
            config_reader = ConfigReader()
            enabled_cameras = config_reader.get_enabled_cameras()
            
            if enabled_cameras and len(enabled_cameras) >= 3:
                for i, camera in enumerate(enabled_cameras[:3], 1):
                    video_paths[i] = camera['rtsp_url']
                    print(f"ğŸ“· æ‘„åƒå¤´ {i}: {camera['name']} - {camera['rtsp_url']}")
                print("âœ… ä½¿ç”¨RTSPæµä½œä¸ºè¾“å…¥")
            else:
                print("âš ï¸  é…ç½®æ–‡ä»¶ä¸­æ‘„åƒå¤´æ•°é‡ä¸è¶³ï¼Œå›é€€åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶")
                raise Exception("é…ç½®ä¸è¶³")
        except Exception as e:
            print(f"âš ï¸  è¯»å–RTSPé…ç½®å¤±è´¥: {e}, ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶")
            RTSP_MQTT_AVAILABLE = False
    
    # å›é€€åˆ°æœ¬åœ°è§†é¢‘æ–‡ä»¶
    if not RTSP_MQTT_AVAILABLE or not video_paths:
        print("ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶")
        # è®¾ç½®é»˜è®¤çš„æœ¬åœ°è§†é¢‘æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨å®é™…å­˜åœ¨çš„è§†é¢‘æ–‡ä»¶
        video_paths = {
            1: "/root/yolov5-7.0_lyngor1.17.0/videos/test_121.mp4",
            2: "/root/yolov5-7.0_lyngor1.17.0/videos/test_122.mp4",
            3: "/root/yolov5-7.0_lyngor1.17.0/videos/test_123.mp4"
        }
        
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for cam_id, video_path in video_paths.items():
            if not os.path.exists(video_path):
                print(f"âŒ Camera{cam_id} è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                print(f"   è¯·ç¡®ä¿è§†é¢‘æ–‡ä»¶å­˜åœ¨æˆ–æä¾›æ­£ç¡®çš„RTSPæºé…ç½®")
                sys.exit(1)
            else:
                print(f"ğŸ“· Camera{cam_id}: {video_path}")
    
    detect_areas = {
        1: [np.array([[0, 720], [226, 324], [576, 77], [714, 77], [1278, 390], [1280, 720]], dtype=np.int32),
            np.array([[218,324], [472,149], [366,141], [48,312]], dtype=np.int32)],
        2: [np.array([[0, 503], [0, 714], [1280, 720], [1280, 410], [800, 128],[471,133]], dtype=np.int32)],
        3: [np.array([[70, 720], [1030, 720], [934, 166], [90, 166]], dtype=np.int32)]}
        
    signal.signal(signal.SIGINT, cancel_process)
    
    # 2. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
    fusion_system = CrossCameraFusion()
    queues = {i: multiprocessing.Queue(maxsize=10) for i in [1, 2, 3]}
    perf_monitor = PerformanceMonitor()
    
    # 2.1 åˆå§‹åŒ–é›·è¾¾èåˆæ¨¡å—
    print("\nğŸ”§ åˆå§‹åŒ–é›·è¾¾èåˆæ¨¡å—...")
    radar_fusion_enabled = False
    radar_fusion_processor = None
    radar_data_loader = None
    
    # é›·è¾¾æ•°æ®æ–‡ä»¶è·¯å¾„ (å¯é…ç½®)
    radar_data_path = 'c:/Users/zhenghuiwen1/Desktop/project_simple/radar_vision/radar_data_85_aligned.jsonl'
    
    # èåˆåŒºåŸŸé…ç½® (å¯é€‰ï¼Œå¦‚æœéœ€è¦åŒºåŸŸè¿‡æ»¤)
    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„èåˆåŒºåŸŸï¼Œæˆ–è€…å¯ä»¥ä¸ºæ¯ä¸ªæ‘„åƒå¤´é…ç½®ä¸åŒçš„åŒºåŸŸ
    fusion_area_geo = [
        [113.583894894, 23.530394880],
        [113.584462681, 23.530850485],
        [113.584032327, 23.530886446],
        [113.583922645, 23.530898319]
    ]
    
    try:
        if os.path.exists(radar_data_path):
            # åˆå§‹åŒ–é›·è¾¾æ•°æ®åŠ è½½å™¨
            radar_data_loader = RadarDataLoader(radar_data_path)
            if radar_data_loader.load():
                # åˆå§‹åŒ–é›·è¾¾èåˆå¤„ç†å™¨
                radar_fusion_processor = RadarVisionFusionProcessor(
                    fusion_area_geo=fusion_area_geo,
                    lat_offset=-0.00000165,
                    lon_offset=0.0000450
                )
                
                # å°†æ‰€æœ‰é›·è¾¾æ•°æ®æ·»åŠ åˆ°å¤„ç†å™¨
                for ts in radar_data_loader.get_all_timestamps():
                    radar_objs = radar_data_loader.get_radar_data(ts)
                    radar_fusion_processor.add_radar_data(ts, radar_objs)
                
                radar_fusion_enabled = True
                print(f"âœ… é›·è¾¾èåˆæ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
                print(f"   é›·è¾¾æ•°æ®å¸§æ•°: {len(radar_data_loader.get_all_timestamps())}")
            else:
                print("âš ï¸  é›·è¾¾æ•°æ®åŠ è½½å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨é›·è¾¾èåˆ")
        else:
            print(f"âš ï¸  é›·è¾¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {radar_data_path}")
            print("   å°†ä¸ä½¿ç”¨é›·è¾¾èåˆåŠŸèƒ½")
    except Exception as e:
        print(f"âš ï¸  é›·è¾¾èåˆæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        print("   å°†ä¸ä½¿ç”¨é›·è¾¾èåˆåŠŸèƒ½")
        radar_fusion_enabled = False
    
    # DEBUG: è·Ÿè¸ªå™¨è¾“å…¥ç»Ÿè®¡
    tracker_input_stats = {1: {'total': 0, 'empty': 0, 'non_empty': 0, 'total_dets': 0}, 
                          2: {'total': 0, 'empty': 0, 'non_empty': 0, 'total_dets': 0},
                          3: {'total': 0, 'empty': 0, 'non_empty': 0, 'total_dets': 0}}
    
    # åˆå§‹åŒ–MQTTå‘å¸ƒå™¨
    mqtt_publisher = None
    if RTSP_MQTT_AVAILABLE:
        try:
            mqtt_publisher = MqttPublisher("config/mqtt_config.ini")
            mqtt_publisher.connect()
            print("âœ… MQTTå‘å¸ƒå™¨å·²è¿æ¥")
        except Exception as e:
            print(f"âš ï¸  MQTTè¿æ¥å¤±è´¥: {e}, å°†ä½¿ç”¨JSONæ–‡ä»¶ä¿å­˜")
            mqtt_publisher = None
    
    class TrackerArgs: # æ–°ByteTracker æ‰€éœ€å‚æ•°
        def __init__(self):
            self.track_thresh = 0.5    # è·Ÿè¸ªç½®ä¿¡åº¦é˜ˆå€¼ï¼Œè¿‡ä½ä¼šå¯¼è‡´å™ªå£°
            self.track_buffer = 30     # å¢åŠ ç¼“å†²åŒºå¤§å°ä»¥ä¿ç•™è½¨è¿¹å†å²
            self.match_thresh = 0.8    # åŒ¹é…é˜ˆå€¼
            self.mot20 = False         # MOT20æ•°æ®é›†æ ‡å¿—
    
    tracker_args = TrackerArgs()
    trackers = {i: BYTETracker(tracker_args, frame_rate=Config.FPS) for i in [1, 2, 3]} # å±€éƒ¨è·Ÿè¸ªå™¨è¿è¡Œåœ¨ä¸»è¿›ç¨‹
    print("âœ… å·²å¯ç”¨ä¼˜åŒ–ç‰ˆByteTracker - äº¤æ›¿è·Ÿè¸ªæ¨¡å¼") 

    # ğŸ”§ æ–°å¢ï¼šRTSPè¿æ¥æµ‹è¯•å‡½æ•°
    def test_rtsp_connection(rtsp_url: str, timeout: int = 5) -> bool:
        """æµ‹è¯•RTSPè¿æ¥æ˜¯å¦å¯ç”¨"""
        try:
            import cv2
            # å°è¯•ä½¿ç”¨ PyAV (SDKinfer_ffmpeg.py çš„ä¾èµ–) æ¥æµ‹è¯•ï¼Œæ›´ä¸€è‡´
            try:
                import av
                av.logging.set_level(av.logging.ERROR)
                container = av.open(rtsp_url, 'r', options={'rtsp_transport': 'tcp', 'stimeout': str(timeout * 1000000)}, timeout=timeout)
                container.decode(video=0)
                container.close()
                return True
            except ImportError:
                # å›é€€åˆ° OpenCV
                cap = cv2.VideoCapture(rtsp_url, cv2.CAP_FFMPEG)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                # è®¾ç½®è¶…æ—¶
                start_time = time.time()
                ret = False
                while time.time() - start_time < timeout:
                    ret, frame = cap.read()
                    if ret:
                        break
                    time.sleep(0.1)
                cap.release()
                return ret
        except Exception as e:
            return False
    
    # ğŸ”§ æ–°å¢ï¼šåœ¨SDKåˆå§‹åŒ–å‰æµ‹è¯•RTSPè¿æ¥
    print("\nğŸ” æµ‹è¯•RTSPè¿æ¥ (ä½¿ç”¨PyAV/OpenCV)...")
    rtsp_connection_status = {}
    for cam_id in [1, 2, 3]:
        video_path = video_paths[cam_id]
        # åªæµ‹è¯•RTSP URLï¼Œä¸æµ‹è¯•æœ¬åœ°æ–‡ä»¶
        if video_path.startswith('rtsp://'):
            print(f"  æµ‹è¯• Camera{cam_id}: {video_path[:60]}...")
            is_connected = test_rtsp_connection(video_path, timeout=5)
            rtsp_connection_status[cam_id] = is_connected
            if is_connected:
                print(f"  âœ… Camera{cam_id} RTSPè¿æ¥æˆåŠŸ")
            else:
                print(f"  âŒ Camera{cam_id} RTSPè¿æ¥å¤±è´¥ - æ— æ³•è¿æ¥åˆ°: {video_path}")
        else:
            # æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•
            rtsp_connection_status[cam_id] = True
            print(f"  âœ… Camera{cam_id} ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶: {video_path}")
    # 3. åˆ›å»ºå¹¶å¯åŠ¨ SDK æ¨ç†è¿›ç¨‹ (ç”Ÿäº§è€…)
    processes = []
    print("\nğŸš€ å¯åŠ¨SDKç‰ˆå¤šæ‘„åƒå¤´èåˆç³»ç»Ÿ")
    
    # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥RTSPè¿æ¥çŠ¶æ€ï¼Œç»™å‡ºè­¦å‘Š
    failed_cameras = [cam_id for cam_id, status in rtsp_connection_status.items() if not status]
    if failed_cameras:
        print(f"âš ï¸  è­¦å‘Š: {len(failed_cameras)}ä¸ªæ‘„åƒå¤´RTSPè¿æ¥æµ‹è¯•å¤±è´¥: {failed_cameras}")
        time.sleep(2)  # ç»™ç”¨æˆ·æ—¶é—´é˜…è¯»è­¦å‘Š
    
    for camera_id in [1, 2, 3]:
        video_path = video_paths[camera_id]
        if not rtsp_connection_status.get(camera_id, True):
            print(f"âš ï¸  Camera{camera_id} RTSPè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œä½†ä»å°è¯•å¯åŠ¨SDK...")
        
        # ğŸ”§ ä¿®æ”¹ï¼šç§»é™¤ timestamp_providers å‚æ•°
        process = multiprocessing.Process(
            target=create_sdk_worker_process,
            args=(camera_id, video_path, queues[camera_id]),
            daemon=True
        )
        processes.append(process)
        process.start()
        print(f"ğŸ”„ å¯åŠ¨Camera{camera_id} SDKæ¨ç†è¿›ç¨‹...")
        
    print("âœ… æ‰€æœ‰SDKæ¨ç†è¿›ç¨‹å·²å¯åŠ¨")

    # 3.5. å¯åŠ¨é¢„çƒ­é˜¶æ®µï¼šç­‰å¾…æ‰€æœ‰æ‘„åƒå¤´é˜Ÿåˆ—éƒ½æœ‰æ•°æ®
    print("\nâ±ï¸  è¿›å…¥é¢„çƒ­é˜¶æ®µï¼Œç­‰å¾…æ‰€æœ‰æ‘„åƒå¤´æ¨é€ç¬¬ä¸€å¸§æ•°æ®...")
    PREHEAT_TIMEOUT = 30  # 30ç§’è¶…æ—¶
    start_time = time.time()
    ready_cameras = {i: False for i in [1, 2, 3]}
    last_report_time = time.time()
    
    while time.time() - start_time < PREHEAT_TIMEOUT:
        all_ready = True
        for cam_id in [1, 2, 3]:
            if not ready_cameras[cam_id] and not queues[cam_id].empty():
                ready_cameras[cam_id] = True
                print(f"âœ… æ‘„åƒå¤´ C{cam_id} å·²å°±ç»ªï¼")
        
        if all(ready_cameras.values()):
            print("ğŸ‰ æ‰€æœ‰æ‘„åƒå¤´å‡å·²å°±ç»ªï¼Œé¢„çƒ­å®Œæˆï¼")
            break
        
        # æ¯5ç§’æ‰“å°ä¸€æ¬¡çŠ¶æ€
        current_time = time.time()
        if current_time - last_report_time >= 5:
            elapsed = int(current_time - start_time)
            queue_sizes = {i: queues[i].qsize() for i in [1, 2, 3]}
            process_status = {i: (p.is_alive() and "âœ“è¿è¡Œä¸­" or "âœ—å·²åœæ­¢") for i, p in enumerate(processes)}
            print(f"â³ é¢„çƒ­è¿›åº¦ ({elapsed}s/{PREHEAT_TIMEOUT}s): é˜Ÿåˆ— {queue_sizes}, è¿›ç¨‹çŠ¶æ€ {[process_status.get(i) for i in range(3)]}")
            print(f"   å‡†å¤‡å°±ç»ª: C1={ready_cameras[1]}, C2={ready_cameras[2]}, C3={ready_cameras[3]}")
            last_report_time = current_time
        
        time.sleep(0.5)
    else:
        print("âŒ é¢„çƒ­è¶…æ—¶ï¼")
        for cam_id, is_ready in ready_cameras.items():
            if not is_ready:
                print(f"  - æ‘„åƒå¤´ C{cam_id} æœªèƒ½åœ¨ {PREHEAT_TIMEOUT} ç§’å†…æ¨é€æ•°æ®ã€‚")
        print("ç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œä½†å¯èƒ½ä¼šå‡ºç°åŒæ­¥é—®é¢˜ã€‚")


    # 4. ä¸»å¾ªç¯ï¼šæ—¶é—´æˆ³èåˆé€»è¾‘ (æ¶ˆè´¹è€…)
    current_frame = 0
    
    # ğŸ”§ æ–°å¢ï¼šè®¾ç½®æ‘„åƒå¤´èµ·å§‹æ—¶é—´æˆ³ï¼ˆç»å¯¹æ—¶é—´æ ¼å¼ï¼‰
    print("\nğŸ”§ é…ç½®æ‘„åƒå¤´æ—¶é—´æˆ³...")
    FFmpegTimeStampProvider.set_all_camera_start_datetimes(Config.CAMERA_START_DATETIMES)
    print("âœ… æ‘„åƒå¤´æ—¶é—´æˆ³é…ç½®å®Œæˆ")
    
    # åˆå§‹åŒ–FFmpegæ—¶é—´æˆ³å¸§åŒæ­¥å™¨
    frame_synchronizer = FFmpegTimestampFrameSynchronizer(
        num_cameras=3, 
        timestamp_tolerance_ms=4000  # å¯åŠ¨å®¹å¿åº¦ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºWarmupé˜¶æ®µå¯¹é½èµ·è·‘çº¿
    )
    # ğŸ”§ æ›´æ–°ï¼šä½¿ç”¨ç»å¯¹æ—¶é—´æˆ³åŒæ­¥ - Warmup + åŠ¨æ€ä¸¢å¼ƒç­–ç•¥
    sync_mode = "ç»å¯¹æ—¶é—´æˆ³åŒæ­¥ - Warmup + åŠ¨æ€ä¸¢å¼ƒç­–ç•¥"
    
    frame_loss_prevention = FrameLossPrevention()
    
    print("\n--- èåˆä¸»å¾ªç¯å¯åŠ¨ ---")
    print(f"ğŸ¯ åŒæ­¥æ¨¡å¼ï¼š{sync_mode}")
    print("="*60)

    try:
        last_sync_report = time.time()
        no_sync_count = 0  # è®¡æ•°è¿ç»­æ²¡æœ‰åŒæ­¥å¸§çš„æ¬¡æ•°
        
        while not cancel_flag.value:
            
            # A. ä»æ‰€æœ‰é˜Ÿåˆ—ä¸­è·å–ç»“æœå¹¶é€å…¥åŒæ­¥å™¨
            perf_monitor.start_timer('queue_processing')
            for camera_id in [1, 2, 3]:
                queue_size = queues[camera_id].qsize()
                perf_monitor.record_queue_stats(camera_id, queue_size, 'read')
                
                while True: # ä½¿ç”¨ get_nowait() å¿«é€Ÿæ¸…ç©ºé˜Ÿåˆ—ï¼Œé¿å…é˜»å¡
                    try:
                        result = queues[camera_id].get_nowait()
                        frame_id = result['frame_id']
                        perf_monitor.add_counter('queue_operations')
                        
                        # é˜²ä¸¢å¸§æ£€æµ‹
                        if frame_loss_prevention.check_frame_sequence(camera_id, frame_id):
                            # æ·»åŠ åˆ°FFmpegæ—¶é—´æˆ³åŒæ­¥å™¨
                            frame_synchronizer.add_frame(camera_id, result)
                        
                    except multiprocessing.queues.Empty:
                        break # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºå†…å±‚å¾ªç¯
                    except Exception as e:
                        print(f"âŒ C{camera_id} é˜Ÿåˆ—è¯»å–å¼‚å¸¸: {e}")
                        break
            
            queue_processing_time = perf_monitor.end_timer('queue_processing')
            
            # B. è·å–æ—¶é—´æˆ³åŒæ­¥çš„å¸§
            synchronized_frames, sync_frame_number = frame_synchronizer.get_synchronized_frames()
            
            if not synchronized_frames:
                # æ²¡æœ‰å¯åŒæ­¥çš„å¸§ï¼ŒçŸ­æš‚ç­‰å¾…ï¼Œé¿å…CPUç©ºè½¬
                no_sync_count += 1
                
                # ğŸ”§ æ”¹è¿›ï¼šæ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                if no_sync_count % 20 == 0:
                    current_time = time.time()
                    buffer_status = frame_synchronizer.get_buffer_status()
                    queue_sizes = {i: queues[i].qsize() for i in [1, 2, 3]}
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å±æ€§åwarmup_complete
                    warmup_complete = getattr(frame_synchronizer, 'warmup_complete', False)
                    warmup_status = "âœ…å®Œæˆ" if warmup_complete else "â³è¿›è¡Œä¸­"
                    
                    print(f"â±ï¸  ç­‰å¾…åŒæ­¥... (è¿ç»­{no_sync_count}ä¸ªå‘¨æœŸ)")
                    print(f"   é˜Ÿåˆ—å¤§å°: C1={queue_sizes[1]}, C2={queue_sizes[2]}, C3={queue_sizes[3]}")
                    print(f"   ç¼“å†²åŒº: {buffer_status}")
                    print(f"   WarmupçŠ¶æ€: {warmup_status}")
                    
                    # æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                    alive_count = sum(1 for p in processes if p.is_alive())
                    print(f"   SDKè¿›ç¨‹: {alive_count}/3 è¿è¡Œä¸­")
                
                # ğŸ”§ æ”¹è¿›ï¼šæ›´é•¿çš„è¶…æ—¶æ—¶é—´ï¼Œå› ä¸ºWarmupé˜¶æ®µå¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´
                if no_sync_count > 500:  # æé«˜åˆ°500ä¸ªå‘¨æœŸï¼ˆçº¦2.5ç§’ï¼‰
                    print(f"\nâŒ è­¦å‘Šï¼šå·²è¿ç»­{no_sync_count}ä¸ªå‘¨æœŸæ— æ³•åŒæ­¥")
                    print(f"   è¯¦ç»†è¯Šæ–­ä¿¡æ¯:")
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å±æ€§åwarmup_complete
                    warmup_complete = getattr(frame_synchronizer, 'warmup_complete', False)
                    print(f"   1. WarmupçŠ¶æ€: {'âœ…å®Œæˆ' if warmup_complete else 'âŒæœªå®Œæˆ'}")
                    if not warmup_complete:
                        print(f"      ç­‰å¾…æ‰€æœ‰æ‘„åƒå¤´å¯¹é½èµ·è·‘çº¿...")
                    buffer_status = frame_synchronizer.get_buffer_status()
                    print(f"   2. ç¼“å†²åŒºçŠ¶æ€: {buffer_status}")
                    queue_sizes = {i: queues[i].qsize() for i in [1, 2, 3]}
                    print(f"   3. é˜Ÿåˆ—å¤§å°: {queue_sizes}")
                    alive_count = sum(1 for p in processes if p.is_alive())
                    print(f"   4. SDKè¿›ç¨‹çŠ¶æ€: {alive_count}/3 è¿è¡Œä¸­")

                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¿›ç¨‹å·²åœæ­¢
                    if alive_count < 3:
                        print("   æ£€æµ‹åˆ°SDKè¿›ç¨‹å·²åœæ­¢ï¼Œä¸»å¾ªç¯é€€å‡ºã€‚")
                        break
                
                # å¦‚æœæ‰€æœ‰è¿›ç¨‹éƒ½åœæ­¢äº†ï¼Œä¹Ÿé€€å‡º
                if all(not p.is_alive() for p in processes):
                    print("âŒ æ‰€æœ‰SDKå­è¿›ç¨‹å·²åœæ­¢ï¼Œä¸»å¾ªç¯é€€å‡ºã€‚")
                    break

                time.sleep(0.005) 
                continue
            
            # é‡ç½®æ— åŒæ­¥è®¡æ•°
            no_sync_count = 0

            # C. åªè¦æœ‰æ•°æ®å°±è¿›è¡Œèåˆ
            current_frame = sync_frame_number
            current_frame_results = synchronized_frames
            # ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ¯å¸§åŒæ­¥å’Œå¤„ç†æƒ…å†µ
            perf_monitor.add_counter('frames_synchronized')
            perf_monitor.add_counter('frames_processed')
                
            all_frame_detections = []
            perf_monitor.start_timer('frame_processing')
            
            for camera_id, result in current_frame_results.items():
                perf_monitor.start_timer(f'camera_{camera_id}_processing')
                
                # 1. ç±»åˆ«è¿‡æ»¤ï¼šåªè·Ÿè¸ªè½¦è¾†ï¼Œæ’é™¤éè½¦è¾†ç±»åˆ«
                raw_detections = [d for d in result['detections'] 
                                if d['class'] in Config.VEHICLE_CLASSES and d['class'] not in Config.EXCLUDE_CLASSES]
                # ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ£€æµ‹æ•°é‡
                perf_monitor.add_counter('detections_processed', len(raw_detections))
                
                # 2. NMS å’Œ è·Ÿè¸ªå™¨è¾“å…¥æ ¼å¼è½¬æ¢
                # æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡æ•°æ®è½¬æ¢ï¼Œé¿å…å¾ªç¯ï¼Œå‡å°‘2-3mså¤„ç†æ—¶é—´
                perf_monitor.start_timer('nms_processing')
                det_for_nms = [{'box': d['box'], 'confidence': d['confidence'], 'class': d['class']} for d in raw_detections]
                nms_detections = DetectionUtils.non_max_suppression(det_for_nms)
                perf_monitor.end_timer('nms_processing')
                
                # 3. åŒºåŸŸè¿‡æ»¤ (å…ˆè¿‡æ»¤åè·Ÿè¸ªï¼Œä¸main_combinedä¿æŒä¸€è‡´) âœ… FIX
                perf_monitor.start_timer('area_filtering')
                filtered_nms_detections = filter_by_detect_areas(nms_detections, detect_areas[camera_id])
                perf_monitor.end_timer('area_filtering')
                
                # 4. ä½¿ç”¨æ‰¹é‡å¤„ç†å‡½æ•°ï¼Œæå‡æ€§èƒ½
                perf_monitor.start_timer('tracker_input_preparation')
                tracker_input_tensor, box_to_class = batch_prepare_tracker_input(filtered_nms_detections)
                perf_monitor.end_timer('tracker_input_preparation')
                
                # é™åˆ¶æœ€å¤§è·Ÿè¸ªç›®æ ‡æ•°é‡ï¼Œé¿å…åŒˆç‰™åˆ©ç®—æ³•æ€§èƒ½é—®é¢˜
                if len(tracker_input_tensor) > 50:  # é™åˆ¶æœ€å¤š50ä¸ªç›®æ ‡
                    tracker_input_tensor = tracker_input_tensor[:50]
                    print(f"âš ï¸  C{camera_id} ç›®æ ‡æ•°é‡è¿‡å¤š({len(filtered_nms_detections)})ï¼Œé™åˆ¶ä¸º50ä¸ª")

                # 5. å±€éƒ¨è·Ÿè¸ª (BYTETracker)
                perf_monitor.start_timer(f'tracker_update_{camera_id}')
                img_info = [Config.IMAGE_HEIGHT, Config.IMAGE_WIDTH]  # [height, width]
                img_size = (Config.IMAGE_HEIGHT, Config.IMAGE_WIDTH)  # (width, height)
                
                # DEBUG: æ£€æŸ¥trackerè¾“å…¥
                debug_input_count = len(tracker_input_tensor) if tracker_input_tensor is not None else 0
                tracker_input_stats[camera_id]['total'] += 1
                tracker_input_stats[camera_id]['total_dets'] += debug_input_count
                if debug_input_count == 0:
                    tracker_input_stats[camera_id]['empty'] += 1
                else:
                    tracker_input_stats[camera_id]['non_empty'] += 1
                
                # æ‰“å°è·Ÿè¸ªå‰çš„è°ƒè¯•ä¿¡æ¯
                # ğŸ”§ ä¼˜åŒ–ï¼šDEBUGè¾“å‡ºä»æ¯100å¸§æ”¹ä¸ºæ¯500å¸§
                if current_frame % 500 == 0:
                    print(f"  ğŸ“Š C{camera_id} F{current_frame} è·Ÿè¸ªå‰: åŸå§‹æ£€æµ‹={len(raw_detections)}, NMSå={len(nms_detections)}, è¿‡æ»¤å={len(filtered_nms_detections)}, è·Ÿè¸ªå™¨è¾“å…¥={debug_input_count}")
                
                tracked_objects = trackers[camera_id].update(tracker_input_tensor, img_info, img_size)
                tracker_time = perf_monitor.end_timer(f'tracker_update_{camera_id}')
                # ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ¯æ¬¡è·Ÿè¸ªå™¨æ›´æ–°
                perf_monitor.add_counter('tracker_updates')
                
                perf_monitor.record_fusion_stats(f'tracker_update_{camera_id}', tracker_time, {
                    'input_count': len(filtered_nms_detections),
                    'output_count': len(tracked_objects)
                })

                # 6. è·Ÿè¸ªç»“æœè½¬æ¢ (æ€§èƒ½ä¼˜åŒ–ç‰ˆ)
                # ä¼˜åŒ–ç‚¹ï¼šä¿ç•™åŸå§‹ç±»åˆ«ä¿¡æ¯ï¼Œæå‡è·Ÿè¸ªç²¾åº¦ï¼Œä½¿ç”¨box_to_classæ˜ å°„ âœ… FIX
                tracked_detections = batch_convert_track_results(tracked_objects, result, camera_id, current_frame, filtered_nms_detections, box_to_class)
                
                # 7. è·¨æ‘„åƒå¤´èåˆå¤„ç† - æ–°çš„èåˆé€»è¾‘
                global_targets, local_targets = fusion_system.process_detections(tracked_detections, camera_id, perf_monitor)
                
                # å­˜å‚¨æ­¤å¸§çš„å…¨å±€å’Œæœ¬åœ°ç›®æ ‡
                all_frame_detections.append({
                    'camera_id': camera_id,
                    'global_targets': global_targets,
                    'local_targets': local_targets
                })
                
                camera_processing_time = perf_monitor.end_timer(f'camera_{camera_id}_processing')
                # ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ¯å¸§çš„æ‘„åƒå¤´å¤„ç†ç»Ÿè®¡
                perf_monitor.record_fusion_stats(f'camera_{camera_id}_processing', camera_processing_time, {
                    'raw_detections': len(raw_detections),
                    'tracked_detections': len(tracked_detections),
                    'filtered_detections': len(filtered_nms_detections),
                    'global_targets': len(global_targets),
                    'local_targets': len(local_targets)
                })
            
            frame_processing_time = perf_monitor.end_timer('frame_processing')
            
            # æ”¶é›†æ‰€æœ‰å…¨å±€å’Œæœ¬åœ°ç›®æ ‡
            all_global_targets = []
            all_local_targets = []
            for frame_data in all_frame_detections:
                all_global_targets.extend(frame_data['global_targets'])
                all_local_targets.extend(frame_data['local_targets'])
            
            # C. è·¨æ‘„åƒå¤´èåˆï¼šåŒ¹é…å…¨å±€å’Œæœ¬åœ°ç›®æ ‡
            perf_monitor.start_timer('matching_processing')
            active_global_targets = list(fusion_system.global_targets.values())
            fusion_system._perform_matching(all_local_targets, active_global_targets, perf_monitor)
            
            # æ›´æ–°å…¨å±€çŠ¶æ€
            fusion_system.update_global_state(all_global_targets, all_local_targets)
            matching_time = perf_monitor.end_timer('matching_processing')

            # D. é›·è¾¾èåˆå¤„ç† (å¼‚æ­¥)
            radar_id_map = {}
            if radar_fusion_enabled and radar_fusion_processor:
                perf_monitor.start_timer('radar_fusion_processing')
                
                # å°†æ‰€æœ‰ç›®æ ‡è½¬æ¢ä¸º OutputObject æ ¼å¼
                vision_objects = []
                
                # å¤„ç†å…¨å±€ç›®æ ‡
                for global_target in all_global_targets:
                    if not global_target.bev_trajectory:
                        continue
                    current_bev = global_target.bev_trajectory[-1]
                    if current_bev[0] == 0.0 and current_bev[1] == 0.0:
                        continue
                    
                    geo_result = GeometryUtils.bev_to_geo(current_bev[0], current_bev[1])
                    if not geo_result:
                        continue
                    
                    lng, lat = geo_result
                    confidence = global_target.confidence_history[-1] if global_target.confidence_history else 0.0
                    
                    vision_obj = OutputObject(
                        timestamp="",  # å°†åœ¨åé¢å¡«å……
                        cameraid=global_target.camera_id,
                        type_name=global_target.class_name,
                        confidence=confidence,
                        track_id=global_target.global_id,
                        lon=lng,
                        lat=lat
                    )
                    vision_objects.append(vision_obj)
                
                # å¤„ç†æœ¬åœ°ç›®æ ‡ (å·²åŒ¹é…çš„)
                for local_target in all_local_targets:
                    if not local_target.matched_global_id:
                        continue
                    
                    if local_target.current_bev_pos[0] == 0.0 and local_target.current_bev_pos[1] == 0.0:
                        continue
                    
                    geo_result = GeometryUtils.bev_to_geo(local_target.current_bev_pos[0], local_target.current_bev_pos[1])
                    if not geo_result:
                        continue
                    
                    lng, lat = geo_result
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ·»åŠ è¿‡è¿™ä¸ª global_id
                    if not any(v.track_id == local_target.matched_global_id for v in vision_objects):
                        vision_obj = OutputObject(
                            timestamp="",
                            cameraid=local_target.camera_id,
                            type_name=local_target.class_name,
                            confidence=local_target.confidence,
                            track_id=local_target.matched_global_id,
                            lon=lng,
                            lat=lat
                        )
                        vision_objects.append(vision_obj)
                
                # æ‰§è¡Œé›·è¾¾èåˆ
                if vision_objects:
                    vision_timestamp = ts  # ä½¿ç”¨å½“å‰å¸§çš„æ—¶é—´æˆ³
                    updated_vision_objects = radar_fusion_processor.process_frame(vision_timestamp, vision_objects)
                    
                    # æ„å»º radar_id_map
                    for vision_obj in updated_vision_objects:
                        if vision_obj.radar_id is not None:
                            radar_id_map[vision_obj.track_id] = vision_obj.radar_id
                    
                    # ç»Ÿè®¡ä¿¡æ¯
                    matched_count = sum(1 for v in updated_vision_objects if v.radar_id is not None)
                    if current_frame % 100 == 0 and matched_count > 0:
                        print(f"ğŸ”— Frame {current_frame}: é›·è¾¾åŒ¹é… {matched_count}/{len(updated_vision_objects)} ä¸ªç›®æ ‡")
                
                perf_monitor.end_timer('radar_fusion_processing')
            
            # E. ç”ŸæˆJSONæ•°æ®å¹¶å°è¯•å‘é€MQTT
            perf_monitor.start_timer('json_mqtt_processing')
            
            perf_monitor.start_timer('json_generation')
            json_data = fusion_system.generate_json_data(all_global_targets, all_local_targets, radar_id_map)
            perf_monitor.end_timer('json_generation')
            
            mqtt_sent = False
            if mqtt_publisher:
                perf_monitor.start_timer('mqtt_publish')
                try:
                    participants = json_data.get('participant', [])
                    mqtt_sent = mqtt_publisher.publish_rsm(participants)
                    if mqtt_sent:
                        # ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šè®°å½•MQTTæˆåŠŸå‘é€
                        perf_monitor.add_counter('mqtt_sends')
                    else:
                        # ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šè®°å½•MQTTå‘é€å¤±è´¥
                        perf_monitor.add_counter('mqtt_failures')
                except Exception as e:
                    print(f"âŒ MQTTå‘é€å¼‚å¸¸: {e}")
                    # ğŸ“Š æ€§èƒ½ç›‘æ§ï¼šè®°å½•MQTTå¼‚å¸¸
                    perf_monitor.add_counter('mqtt_failures')
                finally:
                    perf_monitor.end_timer('mqtt_publish')
            
            # ğŸ”§ ä¿®å¤ï¼šæ— è®ºMQTTæ˜¯å¦æˆåŠŸï¼Œéƒ½ä¿å­˜JSONæ•°æ®ï¼ˆç”¨äºè°ƒè¯•å’Œå¤‡ä»½ï¼‰
            fusion_system.json_output_data.append(json_data)
            
            json_mqtt_time = perf_monitor.end_timer('json_mqtt_processing')
            
            # æ‰“å°å¤„ç†ä¿¡æ¯
            #print(f"âœ… åŒæ­¥èåˆ: Frame {current_frame} | ç›®æ ‡æ•°: {len(all_frame_detections)} | MQTT: {'æˆåŠŸ' if mqtt_sent else 'å¤±è´¥/æœªé…ç½®'}")
            
            fusion_system.next_frame()

            # D. å®šæœŸæŠ¥å‘Šä¸¢å¸§æƒ…å†µ
            if current_frame > 0 and current_frame % 300 == 0:
                missing_report = frame_loss_prevention.get_missing_frames_report()
                if missing_report:
                    print(f"\nğŸ“Š ----- ä¸¢å¸§æŠ¥å‘Š (æˆªè‡³Frame {current_frame}) -----")
                    for cam_id, report in missing_report.items():
                        print(f"  C{cam_id}: ä¸¢å¸§{report['missing_count']}ä¸ª({report['loss_rate']:.2f}%), é‡å¤{report['duplicate_count']}ä¸ª")
                    print("-" * 45)
        
        print("\nğŸ¯ æ‰€æœ‰å¤„ç†å®Œæˆ (æˆ–è¾¾åˆ°æœ€å¤§å¸§æ•°)")
        
        # 5. ä¿å­˜èåˆç»“æœï¼ˆæ­£å¸¸é€€å‡ºæ—¶ï¼‰
        try:
            json_count = len(fusion_system.json_output_data) if fusion_system.json_output_data else 0
            print(f"ğŸ’¾ å‡†å¤‡ä¿å­˜ {json_count} å¸§çš„JSONæ•°æ®...")
            if json_count > 0:
                fusion_system.save_json_data("output_fusion_refactored.json")
            else:
                print("âš ï¸  è­¦å‘Š: JSONæ•°æ®åˆ—è¡¨ä¸ºç©º")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # 6. è¾“å‡ºæœ€ç»ˆåŒæ­¥ç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“Š æœ€ç»ˆåŒæ­¥ç»Ÿè®¡æŠ¥å‘Š:")
        final_stats = frame_loss_prevention.get_statistics()
        
        total_processed = sum(stat['total_processed'] for stat in final_stats.values())
        synchronized_frames_count = fusion_system.frame_count

        print(f"ğŸ“ˆ å¤„ç†æ¦‚å†µ:")
        print(f"  æ€»æ¥æ”¶å¸§æ•° (å„æ‘„åƒå¤´åˆè®¡): {total_processed}å¸§")
        print(f"  æˆåŠŸåŒæ­¥å¹¶å¤„ç†çš„å¸§ç»„: {synchronized_frames_count} ç»„")
        
        if total_processed > 0 and len(final_stats) > 0:
            avg_processed_per_cam = total_processed / len(final_stats)
            sync_rate = (synchronized_frames_count / max(avg_processed_per_cam, 1)) * 100
            print(f"  åŒæ­¥æˆåŠŸç‡ (ä¼°ç®—): {sync_rate:.2f}%")
        
        buffer_status = frame_synchronizer.get_buffer_status()
        remaining_frames = sum(status['count'] for status in buffer_status.values())
        if remaining_frames > 0:
            print(f"âš ï¸  å¤„ç†ç»“æŸæ—¶ç¼“å†²åŒºå‰©ä½™: {remaining_frames}å¸§æœªå¤„ç†")
            
        print("="*60)
        
        # è¾“å‡ºæœ€ç»ˆè·Ÿè¸ªå™¨ä¼˜åŒ–ç»Ÿè®¡
        print("\nğŸ“Š æœ€ç»ˆByteTrackerä¼˜åŒ–ç»Ÿè®¡:")
        print("\nğŸ” DEBUG - è·Ÿè¸ªå™¨è¾“å…¥ç»Ÿè®¡:")
        for cam_id in [1, 2, 3]:
            stats = tracker_input_stats[cam_id]
            avg_dets = stats['total_dets'] / max(stats['total'], 1)
            print(f"  C{cam_id}: è°ƒç”¨{stats['total']}æ¬¡, ç©ºè¾“å…¥{stats['empty']}æ¬¡, éç©º{stats['non_empty']}æ¬¡, æ€»æ£€æµ‹æ•°{stats['total_dets']}, å¹³å‡{avg_dets:.1f}ä¸ª/å¸§")
        print()
        
        for cam_id, tracker in trackers.items():
            stats = tracker.get_performance_stats()
            perf_improvement = stats.get('performance_improvement', 1.0)
            avg_tracking_time = stats.get('avg_tracking_time', 0.0)
            avg_prediction_time = stats.get('avg_prediction_time', 0.0)
            print(f"  C{cam_id}:")
            print(f"    æ€»å¸§æ•°: {stats['total_frames']}")
            print(f"    è·Ÿè¸ªå¸§: {stats['tracking_frames']} ({stats['tracking_frames']/max(stats['total_frames'],1)*100:.1f}%)")
            print(f"    é¢„æµ‹å¸§: {stats['prediction_only_frames']} ({stats['prediction_only_frames']/max(stats['total_frames'],1)*100:.1f}%)")
            print(f"    æ€§èƒ½æå‡: {perf_improvement:.2f}x")
            print(f"    å¹³å‡è·Ÿè¸ªè€—æ—¶: {avg_tracking_time:.3f}s")
            print(f"    å¹³å‡é¢„æµ‹è€—æ—¶: {avg_prediction_time:.3f}s")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ ä¸»ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ğŸ”§ ä¿®å¤ï¼šåœ¨finallyå—ä¸­ä¿å­˜JSONï¼Œç¡®ä¿å³ä½¿å¼‚å¸¸é€€å‡ºä¹Ÿèƒ½ä¿å­˜æ•°æ®
        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜JSONæ•°æ®...")
        try:
            json_count = len(fusion_system.json_output_data) if fusion_system.json_output_data else 0
            print(f"   å‡†å¤‡ä¿å­˜ {json_count} å¸§çš„JSONæ•°æ®")
            if json_count > 0:
                fusion_system.save_json_data("output_fusion_refactored.json")
            else:
                print("âš ï¸  è­¦å‘Š: JSONæ•°æ®åˆ—è¡¨ä¸ºç©ºï¼Œæ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                print(f"   å¯èƒ½åŸå› : 1) ç¨‹åºå¼‚å¸¸é€€å‡º 2) æ²¡æœ‰æ£€æµ‹åˆ°ç›®æ ‡ 3) æ•°æ®æœªæ­£ç¡®æ·»åŠ ")
        except Exception as e:
            print(f"âŒ åœ¨finallyå—ä¸­ä¿å­˜JSONå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        # 7. æ¸…ç†èµ„æº
        cancel_flag.value = True # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åœæ­¢
        for process in processes:
            if process.is_alive():
                process.terminate()
                process.join()

        # æ–­å¼€MQTTè¿æ¥
        if mqtt_publisher:
            try:
                mqtt_publisher.disconnect()
                print("âœ… MQTTè¿æ¥å·²æ–­å¼€")
            except:
                pass
                
        print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")